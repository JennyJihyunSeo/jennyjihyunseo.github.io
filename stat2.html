<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Stat2</title>

<script src="site_libs/header-attrs-2.26/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/jqueryui-1.13.2/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/kePrint-0.0.1/kePrint.js"></script>
<link href="site_libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/all.min.css" rel="stylesheet" />
<link href="site_libs/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet" />

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>






<link rel="stylesheet" href="styles.css" type="text/css" />



<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark the anchor link active (and if it's in a dropdown, also mark that active)
  var dropdown = menuAnchor.closest('li.dropdown');
  if (window.bootstrap) { // Bootstrap 4+
    menuAnchor.addClass('active');
    dropdown.find('> .dropdown-toggle').addClass('active');
  } else { // Bootstrap 3
    menuAnchor.parent().addClass('active');
    dropdown.addClass('active');
  }

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before, .tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "\e259";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "\e258";
  font-family: 'Glyphicons Halflings';
  border: none;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->



<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}

@media print {
.toc-content {
  /* see https://github.com/w3c/csswg-drafts/issues/4434 */
  float: right;
}
}

.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>



</head>

<body>


<div class="container-fluid main-container">


<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Jihyun (Jenny) Seo</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    Stat/ML
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="stat1.html">Customer Segmentation and Predicting Campaign Offer Acceptance</a>
    </li>
    <li>
      <a href="stat3.html">Multiple Linear Regression: California Housing Dataset</a>
    </li>
    <li>
      <a href="stat4.html">Conjoint Analysis</a>
    </li>
    <li>
      <a href="stat5.html">Bootstrap and Monte Carlo Simulation</a>
    </li>
    <li>
      <a href="stat2.html">Statistics using R</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    DB/SQL
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="dbsql1.html">Customer Personality Analysis</a>
    </li>
  </ul>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" data-bs-toggle="dropdown" aria-expanded="false">
    AWS
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li>
      <a href="AWS.html">AWS Fundamentals</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="contact.html">
    <span class="fa fa-sharp fa-solid fa-envelope"></span>
     
  </a>
</li>
<li>
  <a href="files/RESUME_SEO JIHYUN.pdf">
    <span class="ai ai-cv ai-lg"></span>
     
  </a>
</li>
<li>
  <a href="http://github.com/JennyJihyunSeo">
    <span class="fab fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://www.linkedin.com/in/ssue513">
    <span class="fab fa-linkedin-in"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Statistics using R</h1>

</div>


<p><link rel="stylesheet" href="styles.css" type="text/css">
<link rel="stylesheet" href="site_libs/academicons-1.9.1/css/academicons.min.css"/></p>
<p><br><br><br></p>
<p><br><br><br></p>
<div id="hypothesis-testing-with-two-sample-proportion-test"
class="section level2">
<h2>1. Hypothesis Testing with Two-Sample Proportion Test</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Lecture1.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Treatment Effects between New banner Ad and Old Banner Ad]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal" class="section level3">
<h3>2. Goal</h3>
<p>To test if the Click-Through-Rate (CTR) difference between Ad B
(treatment group) and Ad A (Control group).</p>
<p><br></p>
</div>
<div id="hypotheses" class="section level3">
<h3>3. Hypotheses</h3>
<p><strong>Null Hypothesis (H₀):</strong> - The null hypothesis
indicates Ad B is not effective than Ad A - The null hypothesis
indicates the CTR (Click through Rate) of Ad B is less than or equal to
the CTR (Click through Rate) of Ad A <span class="math display">\[
H_0: p_B \leq p_A
\]</span></p>
<p><strong>Alternative Hypothesis (H₁):</strong> - The null hypothesis
indicates Ad B is more effective than Ad A - The null hypothesis
indicates the CTR (Click through Rate) of Ad B is more than the CTR of
Ad A <span class="math display">\[
H_1: p_B &gt; p_A
\]</span></p>
<p><br></p>
</div>
<div id="methodology-summary" class="section level3">
<h3>4. Methodology &amp; Summary</h3>
<ul>
<li>Two Sample proportion z-test was used:
<ul>
<li>Ad A and Ad B each had 500 samples.</li>
<li>Success = clicking the ad (CTR).</li>
<li>CTR of Ad A = 3% CTR and Ad B = 5.6% CTR.</li>
<li>Result: p-value = 0.0307 rejecting the null hypothesis.</li>
</ul></li>
<li>Conclusion: At a 0.05 significance level, the p-value of 0.0307
indicates that the null hypothesis (Ad B is not more effective than Ad
A) can be rejected. The p-value represents the probability of observing
a result as extreme as, or more extreme than, the current outcome under
the assumption that the null hypothesis is true. Since the p-value is
less than the significance level, we have sufficient evidence to reject
the null hypothesis, suggesting that Ad B is statistically significantly
more effective than Ad A.</li>
</ul>
<p><br></p>
</div>
<div id="code" class="section level3">
<h3>5. Code</h3>
<pre class="r"><code>#Q1
### Set seed to ensure we consistently generate the same results
set.seed(1)

### Create two random samples of A and B with sample size 500 respectively 
### A and B are distributed binomial with sample means 0.03 and 0.05
### A and B follow binomial distributions while indicating 1 is success (clicking the Ad) and 0 is failure (not clicking the Ad)
nbin &lt;- 500
A &lt;- data.frame(CTR = rbinom(n = nbin, prob = 0.03, size = 1))
B &lt;- data.frame(CTR = rbinom(n = nbin, prob = 0.05, size = 1))

### Sample counts, counting the success of clicking the Ad.
countA &lt;- sum(A$CTR)
countB &lt;- sum(B$CTR)

### Two-sample proportions z-test with one-sided test 
prop.test(x = c(countB, countA), 
          n = c(nbin, nbin), 
          alternative = &quot;greater&quot;)</code></pre>
<pre><code>## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(countB, countA) out of c(nbin, nbin)
## X-squared = 3.4993, df = 1, p-value = 0.0307
## alternative hypothesis: greater
## 95 percent confidence interval:
##  0.00294022 1.00000000
## sample estimates:
## prop 1 prop 2 
##  0.056  0.030</code></pre>
<pre class="r"><code># Ad B is clicked 5% otherwise Ad A is clicked 3% on the time but this is very unlikely (only 3% based on the p-value) that 
# this outcome can be gained under the assumption that the null hypothesis is true. Therefore, we suspect that the null hypothesis is not true.</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="hypothesis-testing-with-10000-random-samples"
class="section level2">
<h2>2. Hypothesis Testing with 10,000 Random Samples</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-1" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Lecture1.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Treatment Effects between New banner Ad and Old Banner Ad]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-1" class="section level3">
<h3>2. Goal</h3>
<p>To test if the mean Click-Through-Rate (CTR) between Ad A and Ad B is
greater than 2.6%.</p>
<p><br></p>
</div>
<div id="hypotheses-1" class="section level3">
<h3>3. Hypotheses</h3>
<p><strong>Null Hypothesis (H₀):</strong></p>
<p>The mean difference in CTR between Ad B and Ad A is <strong>less than
or equal to 2.6%</strong>:</p>
<p><span class="math display">\[
H_0: \mu_B - \mu_A \leq 0.026
\]</span></p>
<p><strong>Alternative Hypothesis (H₁):</strong></p>
<p>The mean difference in CTR between Ad B and Ad A is <strong>greater
than 2.6%</strong>:</p>
<p><span class="math display">\[
H_1: \mu_B - \mu_A &gt; 0.026
\]</span></p>
<p><br></p>
</div>
<div id="methodology-summary-1" class="section level3">
<h3>4. Methodology &amp; Summary</h3>
<ul>
<li>Two Sample proportion z-test was used:
<ul>
<li>10,000 random samples of size 500 were drawn for both Ad A and Ad B,
assuming both had a mean CTR of 4.3%, which indicates that those samples
are simulated under the assumption that the null hypothesis is true as
the mean of each distribution is the same as each other.</li>
<li>The mean CTR difference was calculated.</li>
<li>If the mean difference exceeded 2.6%, the null hypothesis was
rejected.</li>
<li>Result: p-value = 0.0242 rejecting the null hypothesis.</li>
</ul></li>
<li>Conclusion: At a 0.05 significance level, the p-value of 0.0242
indicates that the null hypothesis (The mean difference between the two
Ads is less than or equal to 2.6%.) can be rejected. The p-value
represents the probability of observing a difference as extreme as, or
more extreme than, 2.6% under the assumption that the null hypothesis is
true. The probability of observing this extreme value (the mean
difference between the two Ads greater than 2.6%) is 2.4%. Since the
p-value is less than the significance level, we have sufficient evidence
to reject the null hypothesis, suggesting that Ad B is statistically
significantly 2.6% more effective than Ad A.</li>
</ul>
<p><br></p>
</div>
<div id="code-1" class="section level3">
<h3>5. Code</h3>
<pre class="r"><code>### 10,000 random samples from binomial with mean 0.043, for both Ad A and AD B which fits the null hypothesis with Ad B not being more effective than Ad A
### 4.3% indicates the average CTR of A and B 
### The purpose for this sampling is to identify whether CTR for Ad B is 2.6% larger than Ad A even if we got random samples for binomial distribution with the same average mean for both Ad A and Ad B. 

set.seed(1)
### Create data frame for mean of samples 
hypothesis &lt;- data.frame(matrix(0, nrow = 10000, ncol = 4))

### Name columns 
colnames(hypothesis) &lt;- c(&#39;A&#39;, &#39;B&#39;, &#39;Dif&#39;, &#39;RejectH0&#39;)

for (x in 1:10000) {
  # The sample mean is the same as 4.3% (CTR = 4.3%)
  hypothesis$A[x] &lt;- mean(rbinom(n = nbin, prob = 0.043, size = 1))
  hypothesis$B[x] &lt;- mean(rbinom(n = nbin, prob = 0.043, size = 1))
  
  # The difference in CTR between A and B
  hypothesis$Dif[x] &lt;- hypothesis$B[x] - hypothesis$A[x]
  
  # If the CTR of B is greater than 2.6% than A, we reject the null hypothesis.
  hypothesis$RejectH0[x] &lt;- ifelse(hypothesis$B[x] - hypothesis$A[x] &gt; 0.026, 1, 0)
}

# The summary table of the result for this hypothesis testing.
summary(hypothesis$RejectH0)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##  0.0000  0.0000  0.0000  0.0242  0.0000  1.0000</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="random-sampling" class="section level2">
<h2>3. Random Sampling</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-2" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/RandomSampling.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. How Sample Mean Evolves as Sample Size Increases]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-2" class="section level3">
<h3>2. Goal</h3>
<ul>
<li>To demonstrate how the sample mean evolves as the sample size
increases, and how the population variance affects the variability of
sample means.
<ul>
<li>The sample mean becomes closer to the true population mean as the
sample size increases.</li>
<li>The stability of sample means improves when sampling from
populations with lower variance.</li>
</ul></li>
</ul>
<p><br></p>
</div>
<div id="methodology" class="section level3">
<h3>3. Methodology</h3>
<ol style="list-style-type: decimal">
<li>Setup:
<ul>
<li>Two populations are sampled, both with a mean of 70 but with
different variances.
<ul>
<li>Population 1: Variance = 36 (Standard Deviation = 6)</li>
<li>Population 2: Variance = 9 (Standard Deviation = 3)</li>
</ul></li>
<li>Sample sizes range from 1 to 100.</li>
</ul></li>
<li>Process:
<ul>
<li>Random samples are drawn from each population for each sample size
(1 to 100).</li>
<li>The sample means are calculated and stored for both
populations.</li>
</ul></li>
<li>Process:
<ul>
<li>The sample means are plotted against sample sizes to observe
trends.</li>
<li>Red Line: Population with larger variance (36)</li>
<li>Blue Line: Population with smaller variance (9)</li>
</ul></li>
<li>Conclusion:
<ul>
<li>When the population variance is larger and sampling from this
population, the sample means vary largely when the sample size is
smaller. This variance is getting smaller when the sample size
increases. In contrast, if the population variance is smaller and random
samples from this population, there is less variability in the sample
means compared to the first random sampling from the population with
larger variance. The sample mean of the second population gets closer to
the population mean which is 70 rapidly while showing greater stability
even when the sample size is small.</li>
</ul></li>
</ol>
<p><br></p>
</div>
<div id="code-2" class="section level3">
<h3>4. Code</h3>
<pre class="r"><code>### Set seed to ensure we consistently generate the same results 
set.seed(1)

### Create data frame to store the means of the samples
sample_example &lt;- data.frame(matrix(0, nrow = 100, ncol = 3))

### Name columns
colnames(sample_example) &lt;- c(&#39;ssize&#39;, &#39;mean1&#39;, &#39;mean2&#39;)

# Create random samples from a normal distribution with mean = 70 but with different variance
# Sample sizes range from 1 to 100

for (x in 1:100) {
  # Generate random samples for two sets from the normal distribution with the same mean 70 and different variances. The first sample has 70 as mean and 36 as variance. The second sample has 70 as mean and 9 as variance. 
  sample_run1 &lt;- data.frame(sample = rnorm(x, 70, 36))
  sample_run2 &lt;- data.frame(sample = rnorm(x, 70, 9))
  
  # Store the sample size
  sample_example$ssize[x] &lt;- x
  
  # Calculate the means of the samples and store them 
  sample_example$mean1[x] &lt;- mean(sample_run1$sample)
  sample_example$mean2[x] &lt;- mean(sample_run2$sample)
}

# View the results
head(sample_example)</code></pre>
<pre><code>##   ssize    mean1    mean2
## 1     1 47.44766 71.65279
## 2     2 83.67374 67.79068
## 3     3 91.61842 74.78871
## 4     4 54.19651 75.27123
## 5     5 72.92430 66.86281
## 6     6 68.78623 71.48757</code></pre>
<pre class="r"><code># When the sample size is the same in each row, if the standard deviation is smaller, we could get more stable sample means closer to the population mean 70.</code></pre>
<pre class="r"><code>### Plot X=Sample Size, Y = mean
library(ggplot2)
ggplot() + 
  geom_line(data=sample_example, aes(x=ssize,y=mean1), color=&quot;red&quot;)+ 
  geom_line(data=sample_example, aes(x=ssize,y=mean2), color=&quot;blue&quot;)+ 
  ggtitle(&quot;How sample mean evolves as sample size increases&quot;) +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x=&#39;Sample Size&#39;,y=&#39;Sample Mean&#39;)</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-4-1.png" width="672" />
<br></p>
<p><br><br><br></p>
</div>
</div>
<div id="multi-armed-bandit" class="section level2">
<h2>4. Multi-Armed Bandit</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="definition" class="section level3">
<h3>1. Definition</h3>
<ul>
<li>The Multi-Armed Bandit approach tests multiple levers (e.g.,
advertisements) to identify the best lever offering the highest
compensation. In this case:
<ul>
<li>4 advertisements are tested to determine the one with the highest
CTR (Click-Through Rate).</li>
<li>Fisher’s exact test is used instead of the proportion test when
sample sizes (exposures of advertisements) are low.</li>
</ul></li>
</ul>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-3" class="section level3">
<h3>2. Goal</h3>
<p>To identify the advertisement with the highest CTR and evaluate
whether any other advertisements can serve as suitable alternatives.</p>
<p><br></p>
</div>
<div id="methodology-1" class="section level3">
<h3>3. Methodology</h3>
<ol style="list-style-type: decimal">
<li>Setup:
<ul>
<li>Four advertisements (Ad1, Ad2, Ad3, Ad4) are initialized with
baseline CTRs:
<ul>
<li>Ad1: 2.5%, Ad2: 1%, Ad3: 0.5%, Ad4: 1.5%.</li>
</ul></li>
<li>The best advertisement starts with the highest baseline CTR.</li>
</ul></li>
<li>Dynamic Sampling:
<ul>
<li>Advertisements are exposed probabilistically, with higher
probabilities assigned to the current best-performing ad.</li>
<li>A random click (1 for success, 0 for failure) is generated based on
the CTR of the selected ad.</li>
</ul></li>
<li>Updating Beliefs:
<ul>
<li>Clicks and exposures are updated after each trial.</li>
<li>CTR for each advertisement is recalculated:
<ul>
<li><strong>CTR</strong> (Click-Through Rate):</li>
</ul></li>
</ul></li>
</ol>
<p><span class="math display">\[
\text{CTR} = \frac{\text{Clicks}}{\text{Exposures}}
\]</span> + CTR represents the ratio of the number of clicks to the
number of times an advertisement was shown (exposures).</p>
<ol start="4" style="list-style-type: decimal">
<li>Testing Significance:
<ul>
<li>Once all advertisements have at least 10 exposures, Fisher’s test is
applied:</li>
<li><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong></li>
</ul>
<span class="math display">\[
H_0: \text{CTR}_{\text{Best}} \geq \text{CTR}_{\text{Alternative}}
\]</span>
<ul>
<li><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong></li>
</ul>
<span class="math display">\[
H_1: \text{CTR}_{\text{Best}} &lt; \text{CTR}_{\text{Alternative}}
\]</span>
<ul>
<li>Advertisements with p-values &lt; 0.05 are removed due to
significant differences from the best ad.</li>
</ul></li>
<li>Conclusion:
<ul>
<li>The process continues until only the best advertisement and its
close alternatives (if any) remain.</li>
<li>The first advertisement has the highest CTR and there are no
suitable alternatives with similar CTR compared to the best choice.</li>
</ul></li>
</ol>
<p><br></p>
</div>
<div id="code-3" class="section level3">
<h3>4. Code</h3>
<pre class="r"><code># Choose 4 advertisements 
N_ARMS = 4
# Identifier for each advertisement 
ARMS = c(1,2,3,4)
# Set the baseline CTR for each advertisement 
base_rates = c(.025,.01,.005,.015)
# Before the experiment, the first advertisement has the highest CTR
best = 1
# Initialize the number of clicks, the number of advertisement exposure, and CTR (the number of clicks / the total number of advertisement exposure)
clicks &lt;-c(0,0,0,0)
draws &lt;-c(0,0,0,0)
CTR &lt;-c(0,0,0,0)

options(warn=-1)

set.seed(1)

while(N_ARMS&gt;1){
  # Generate a random draw of ad from proportion .5 best, else other)
  # Set arm probabilities
  # Except for the best advertisement, 0.5 probabilities divided by remaining advertisements
  epsilon = .5/(N_ARMS-1)
  chosen_arm_p = c(0,0,0,0)
  for (i in 1:length(ARMS)) {
    chosen_arm_p[ARMS[i]]=epsilon
  }
  # Assign 0.5 exposure probability for the best advertisement
  chosen_arm_p[best]=.5
  # Choose the probability as the threshold to pull the arm
  choicep &lt;- runif(1, min = 0, max = 1)
  choice=0
  # Choose the advertisement when the choice probability generated by uniform distribution is included in the cumulative range for each advertisement
  # If one of the advertisements is selected, choice == 0 is not satisfied and the for loop is ended. 
  for (i in 1:length(ARMS)) {
    if (choicep&lt;=sum(chosen_arm_p[1:ARMS[i]]) &amp; choice==0){choice=ARMS[i]}
  }
  # Generate click or not click using CTR for each advertisement set above
  # click has 1 (click) or 0 (not click) for one exposure 
  click &lt;- rbinom(n=1,prob=base_rates[choice],size=1)
  # Update beliefs about frequency
  # Accumulate the number of clicks for each advertisement 
  clicks[choice] &lt;- clicks[choice]+click
  # The number of exposure increments by 1 for every trial 
  draws[choice] &lt;- draws[choice]+1
  # If the exposure is 0, the value is returned as 0 whereas CTR is calculated as clicks[choice] / draws [choice]
  CTR[choice] = ifelse(draws[choice]==0, 0, (clicks[choice]/draws[choice]))
  # Re-assign best 
  best=0
  # CTR object stores the CTR for each advertisement and find the best advertisement with the max CTR value
  for (i in 1:length(ARMS)) {
    if (CTR[ARMS[i]]==max(CTR) &amp; best==0){best=ARMS[i]}
  }
  # Test for significant difference between the best advertisement and the rest of the options
  # Remove all for which p-value &lt;0.05, indicating that there is significant difference between the best and other option
  # Only remove once each alternative has been drawn at least ten times 
  # prop.test inappropriate and gives error if samples are too small
  # The minimum exposure for each advertisement is greater than 10
  if (min(draws)&gt;10) {
  keep = c()
  for (i in 1:length(ARMS)) {
    if (ARMS[i]!=best) {
      # The null hypothesis for the prop.test is CTR(best) is greater or than or equal to CTR(alternative) and the alternative hypothesis is CTR(best) is less than CTR(alternative). 
      # If the null hypothesis is rejected, that alternative should be removed because there is statistically significant difference between the two. However, in this example. prop.test cannot be used due to lower exposure. Chi-squared test may not be appropriate when numerator is zero or very low.
      # Fisher&#39;s test should be used 
    result&lt;-fresult &lt;- fisher.test(
  matrix(
    c(clicks[best], clicks[ARMS[i]], 
      draws[best] - clicks[best], draws[ARMS[i]] - clicks[ARMS[i]]),
    nrow = 2
  ),
  alternative = &quot;greater&quot;
)
    if (result$p.value&gt;.05) {keep=c(keep,ARMS[i])}
    }
    if (ARMS[i]==best) {keep = c(keep,best)}
  }
  N_ARMS = length(keep)
  ARMS=keep
  }
}

ARMS  </code></pre>
<pre><code>## [1] 1</code></pre>
<pre class="r"><code>rbind(base_rates, CTR)</code></pre>
<pre><code>##                  [,1] [,2]        [,3]      [,4]
## base_rates 0.02500000 0.01 0.005000000 0.0150000
## CTR        0.02311007 0.00 0.003984064 0.0158883</code></pre>
<pre class="r"><code>draws</code></pre>
<pre><code>## [1] 2553  151  251 2077</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="instagram-ads-ab-testing-disney" class="section level2">
<h2>5. Instagram Ads A/B Testing (Disney)</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-3" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Disney.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Decide whether to continue on the Ad based on Incremental
Conversion Rate]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-4" class="section level3">
<h3>2. Goal</h3>
<p>To identify whether any treatment effect represented by Incremental
Conversion Rate (ICR) is positive.</p>
<p><br></p>
</div>
<div id="hypotheses-and-formula-of-icr" class="section level3">
<h3>3. Hypotheses and Formula of ICR</h3>
<ul>
<li><p>The <strong>Incremental Conversion Rate (ICR)</strong> is defined
as:</p>
<ul>
<li><strong>ICR</strong> (Incremental Conversion Rate):</li>
</ul>
<p><span class="math display">\[
\text{ICR} = \text{Conversion_Rate(Treatment)} -
\text{Conversion_Rate(Control)}
\]</span></p></li>
<li><p><strong>Treatment group</strong>: Exposed to the
advertisement.<br />
</p></li>
<li><p><strong>Control group</strong>: Not exposed to the
advertisement.</p></li>
<li><p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong></p>
<p><span class="math display">\[
H_0: \text{ICR} \leq 0 \quad \text{(The incremental conversion rate is
less than or equal to 0)}.
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong></p>
<p><span class="math display">\[
H_1: \text{ICR} &gt; 0 \quad \text{(The incremental conversion rate is
greater than 0)}.
\]</span></p></li>
</ul>
</div>
<div id="conclusion" class="section level3">
<h3>4. Conclusion:</h3>
<ul>
<li>The p-value obtained from this test allow us to reject the null
hypothesis. This indicates that the conversion rate in the treatment
group is statistically significantly greater than that in the control
group.</li>
</ul>
<p><br></p>
</div>
<div id="code-4" class="section level3">
<h3>5. Code</h3>
<pre class="r"><code># Load the RData file
load(&quot;DisneyAB.Rdata&quot;) 
### tests
### Proportion test for ICR
# Sample is 10,000 people for treatment group and control group 
sample=10000
# 
prop.test(x=c(sum(DisneyAB$TreatConv),sum(DisneyAB$ControlConv)), n=c(sample,sample), alternative = &quot;greater&quot;)</code></pre>
<pre><code>## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(sum(DisneyAB$TreatConv), sum(DisneyAB$ControlConv)) out of c(sample, sample)
## X-squared = 6.1471, df = 1, p-value = 0.006581
## alternative hypothesis: greater
## 95 percent confidence interval:
##  0.0007070449 1.0000000000
## sample estimates:
## prop 1 prop 2 
## 0.0047 0.0025</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="instagram-ads-roi-return-on-investment-disney"
class="section level2">
<h2>6. Instagram Ads ROI (Return On Investment) (Disney)</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-4" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/AloA.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Calculate ROI to decide whether to continue on the Advertisement.]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-5" class="section level3">
<h3>2. Goal</h3>
<p>To identify whether the conversion revenue is different between
converted treatment and control group. In addition, Return On Investment
(ROI) is calculated to decide whether to continue on the
advertisement.</p>
<p><br></p>
</div>
<div id="hypotheses-and-formula-of-roi" class="section level3">
<h3>3. Hypotheses and Formula of ROI:</h3>
<ul>
<li><p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong></p>
<p><span class="math display">\[
H_0: \mu_{\text{Treatment}} = \mu_{\text{Control}}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong></p>
<p><span class="math display">\[
H_1: \mu_{\text{Treatment}} \neq \mu_{\text{Control}}
\]</span></p></li>
<li><p>The <strong>ROI</strong> is calculated as follows: Sample 10,000
people for treatment and control group. Total cost of experiment is
<span class="math inline">\(10,000 \times 0.2 = 20\)</span>.</p>
<p><span class="math display">\[
\text{ROI} = \frac{\text{Revenue}_{\text{Treatment}} -
\text{Revenue}_{\text{Control}} - \text{Cost}}{\text{Cost}}
\]</span></p></li>
</ul>
</div>
<div id="conclusion-1" class="section level3">
<h3>4. Conclusion:</h3>
<ul>
<li><p>At 0.05 significance level, the null hypothesis cannot be
rejected, indicating there is no statistically significant difference in
conversion revenue between the converted control and treatment group
exposed to Disney advertisement. Consequently, the experiment does not
provide sufficient evidence to conclude that the Disney advertisement
has a significant impact on the average conversion revenue among
converted users.</p></li>
<li><p>The Disney advertisement generates 305.95 times the return on
investment compared to the cost of running the advertisement, relative
to the control group.</p></li>
</ul>
<p><br></p>
</div>
<div id="code-5" class="section level3">
<h3>5. Code</h3>
<pre class="r"><code>### Subset the data
DisTRev &lt;- subset(DisneyAB$TreatRev, DisneyAB$TreatConv == 1)
DisCRev &lt;- subset(DisneyAB$ControlRev, DisneyAB$ControlConv == 1)

### Test Welch Two Sample t-test under the assumption that the variances between the two differ
t.test(DisTRev, DisCRev, alternative = &quot;two.sided&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  DisTRev and DisCRev
## t = -0.34404, df = 40.45, p-value = 0.7326
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1068.5255   757.5723
## sample estimates:
## mean of x mean of y 
##  455.7234  611.2000</code></pre>
<pre class="r"><code>### ROI Disney 
(sum(DisTRev)-sum(DisCRev)-20)/20</code></pre>
<pre><code>## [1] 305.95</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="instagram-ads-ab-testing-alo" class="section level2">
<h2>7. Instagram Ads A/B Testing (Alo)</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-5" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/AloA.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Decide whether to continue on the Ad based on Incremental
Conversion Rate]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-6" class="section level3">
<h3>2. Goal</h3>
<p>To identify whether any treatment effect represented by Incremental
Conversion Rate (ICR) is positive.</p>
<p><br></p>
</div>
<div id="hypotheses-and-formula-of-icr-1" class="section level3">
<h3>3. Hypotheses and Formula of ICR</h3>
<ul>
<li><p>The <strong>Incremental Conversion Rate (ICR)</strong> is defined
as:</p>
<ul>
<li><p><strong>ICR</strong> (Incremental Conversion Rate):</p>
<p><span class="math display">\[
\text{ICR} = \text{Conversion\_Rate(Treatment)} -
\text{Conversion\_Rate(Control)}
\]</span></p></li>
<li><p><strong>Treatment group</strong>: Exposed to the
advertisement.<br />
</p></li>
<li><p><strong>Control group</strong>: Not exposed to the
advertisement.</p></li>
</ul></li>
<li><p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong></p>
<p><span class="math display">\[
H_0: \text{ICR} \leq 0 \quad \text{(The incremental conversion rate is
less than or equal to 0)}.
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong></p>
<p><span class="math display">\[
H_1: \text{ICR} &gt; 0 \quad \text{(The incremental conversion rate is
greater than 0)}.
\]</span></p></li>
</ul>
</div>
<div id="conclusion-2" class="section level3">
<h3>4. Conclusion:</h3>
<ul>
<li>The p-value obtained from this test allow us to reject the null
hypothesis. This indicates that the conversion rate in the treatment
group is statistically significantly greater than that in the control
group.</li>
</ul>
<p><br></p>
</div>
<div id="code-6" class="section level3">
<h3>5. Code</h3>
<pre class="r"><code>load(&quot;AloAB.Rdata&quot;) 
### tests for another data
### Proportion test for ICR
### ICR = Conversion_Rate(Treatment) − Conversion_Rate(Control)
# Assign 10,000 units for both treatment and control groups 
sample=10000
prop.test(x=c(sum(AloAB$TreatConv),sum(AloAB$ControlConv)), n=c(sample,sample), alternative = &quot;greater&quot;)</code></pre>
<pre><code>## 
##  2-sample test for equality of proportions with continuity correction
## 
## data:  c(sum(AloAB$TreatConv), sum(AloAB$ControlConv)) out of c(sample, sample)
## X-squared = 17.926, df = 1, p-value = 1.149e-05
## alternative hypothesis: greater
## 95 percent confidence interval:
##  0.002874909 1.000000000
## sample estimates:
## prop 1 prop 2 
## 0.0086 0.0038</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="instagram-ads-roi-return-on-investment-alo"
class="section level2">
<h2>8. Instagram Ads ROI (Return On Investment) (Alo)</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-6" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/AloA.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Calculate ROI to decide whether to continue on the Advertisement]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="goal-7" class="section level3">
<h3>2. Goal</h3>
<p>To identify whether the conversion revenue is different between
converted treatment and control group. In addition, Return On Investment
(ROI) is calculated to decide whether to continue on the
advertisement.</p>
<p><br></p>
</div>
<div id="hypotheses-and-formula-of-roi-1" class="section level3">
<h3>3. Hypotheses and Formula of ROI:</h3>
<ul>
<li><p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong></p>
<p><span class="math display">\[
H_0: \mu_{\text{Treatment}} = \mu_{\text{Control}}
\]</span></p></li>
<li><p><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong></p>
<p><span class="math display">\[
H_1: \mu_{\text{Treatment}} \neq \mu_{\text{Control}}
\]</span></p></li>
<li><p>The <strong>ROI</strong> is calculated as follows:<br />
Sample 10,000 people for the treatment and control groups.<br />
Total cost of the experiment is <span class="math inline">\(10,000
\times 0.2 = \$20\)</span>.</p>
<p><span class="math display">\[
\text{ROI} = \frac{\text{Revenue}_{\text{Treatment}} -
\text{Revenue}_{\text{Control}} - \text{Cost}}{\text{Cost}}
\]</span></p></li>
</ul>
</div>
<div id="conclusion-3" class="section level3">
<h3>4. Conclusion:</h3>
<ul>
<li><p>At 0.05 significance level, the null hypothesis cannot be
rejected, indicating there is no statistically significant difference in
conversion revenue between the converted control and treatment group
exposed to Alo advertisement. Consequently, the experiment does not
provide sufficient evidence to conclude that the Alo advertisement has a
significant impact on the average conversion revenue among converted
users.</p></li>
<li><p>In case of ROI of the Alo advertisement, the treatment group
exposed to the advertisement 0.3655 times the return on investment
compared to the cost of running the advertisement, relative to the
control group. This indicates that the Alo advertisement is less
effective to generate more revenue from the treatment group compared to
the control group.</p></li>
</ul>
<p><br></p>
</div>
<div id="code-7" class="section level3">
<h3>5. Code</h3>
<pre class="r"><code>AloTRev &lt;- subset(AloAB$TreatRev, AloAB$TreatConv == 1)
AloCRev &lt;- subset(AloAB$ControlRev, AloAB$ControlConv == 1)
t.test(AloTRev, AloCRev, alternative = &quot;two.sided&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  AloTRev and AloCRev
## t = -0.35691, df = 58.769, p-value = 0.7224
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##  -1.3206978  0.9209059
## sample estimates:
## mean of x mean of y 
## 0.7272093 0.9271053</code></pre>
<pre class="r"><code>(sum(AloTRev)-sum(AloCRev)-20)/20</code></pre>
<pre><code>## [1] 0.3655</code></pre>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="hypothesis-testing-with-multiple-treatments"
class="section level2">
<h2>9. Hypothesis Testing with Multiple Treatments</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-7" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Nutrition.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Do Nutrition Labels Encourage Healthier Eating?]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="objective" class="section level3">
<h3>2. Objective</h3>
<p>To determine if there is any difference in the average nutrition
score of shopping baskets across different labeling treatments for an
online grocer. The treatments include: - <strong>Star Nutrition
Labels</strong> - <strong>Letter Grade Nutrition Labels</strong> -
<strong>Warning Labels</strong> - <strong>Control Group (No
Labels)</strong></p>
<p>Dependent variable: Average nutrition score of the shopping
basket.</p>
<p><br></p>
</div>
<div id="methodology-2" class="section level3">
<h3>3. Methodology</h3>
<div id="a-visualization" class="section level4">
<h4>(a) Visualization</h4>
<p>Using boxplots to compare the basket scores across treatments.</p>
<pre class="r"><code>getwd()</code></pre>
<pre><code>## [1] &quot;C:/Github/jennyjihyunseo.github.io&quot;</code></pre>
<pre class="r"><code>file_path &lt;- &quot;C:/Users/sjh50/OneDrive/문서/UC Davis/Paul/Github/Lecture2/Grocery.Rdata&quot;
load(&quot;Grocery.Rdata&quot;)

library(DescTools)
library(ggplot2)</code></pre>
<pre class="r"><code>summary(Grocery$BasketScore[Grocery$Condition==&quot;Control&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   18.90   47.55   54.56   55.27   63.14  100.00</code></pre>
<pre class="r"><code>summary(Grocery$BasketScore[Grocery$Condition==&quot;Star&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   22.04   48.52   57.74   57.45   66.27   94.67</code></pre>
<pre class="r"><code>summary(Grocery$BasketScore[Grocery$Condition==&quot;Letter&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   19.46   50.02   57.86   58.46   67.36  100.00</code></pre>
<pre class="r"><code>summary(Grocery$BasketScore[Grocery$Condition==&quot;Warn&quot;])</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   25.33   52.95   60.77   60.65   69.81   98.85</code></pre>
<pre class="r"><code>### ggplot 
ggplot(Grocery, aes(x=Condition, y=BasketScore, fill=Condition)) + 
geom_boxplot()+
 theme(text = element_text(size = 20))</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
</div>
<div id="b-anova-test" class="section level4">
<h4>(b) ANOVA Test</h4>
<p>To test for significant differences in means among treatments.</p>
<pre class="r"><code>### ANOVA test to identify whether there is significant difference in the mean among different treatments 
anova &lt;-aov(BasketScore ~ Condition, data = Grocery)
summary(anova)</code></pre>
<pre><code>##               Df Sum Sq Mean Sq F value   Pr(&gt;F)    
## Condition      3   7479  2493.0   16.09 2.45e-10 ***
## Residuals   1996 309221   154.9                     
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Star}} = \mu_{\text{Letter}} = \mu_{\text{Warning}} =
\mu_{\text{Control}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>: <span
class="math display">\[
H_A: \text{At least one group mean is different.}
\]</span></p>
<p>At 0.05 significance level, the null hypothesis can be rejected. It
concludes that at least one treatment mean is different. In order to
find which treatment differs, TukeyHSD is used for finding.</p>
</div>
<div id="c-tukeyhsd-test" class="section level4">
<h4>(c) TukeyHSD Test</h4>
<p>To identify which treatments differ.</p>
<pre class="r"><code>TukeyHSD(anova)</code></pre>
<pre><code>##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = BasketScore ~ Condition, data = Grocery)
## 
## $Condition
##                     diff        lwr      upr     p adj
## Letter-Control  3.189506  1.1654647 5.213548 0.0003077
## Star-Control    2.178433  0.1543914 4.202475 0.0291183
## Warn-Control    5.375293  3.3512514 7.399335 0.0000000
## Star-Letter    -1.011073 -3.0351150 1.012968 0.5729945
## Warn-Letter     2.185787  0.1617451 4.209828 0.0283479
## Warn-Star       3.196860  1.1728183 5.220902 0.0002959</code></pre>
<p>Interpretation: (1) Letter - Control: There is a statistically
significant difference in basket score between Letter Nutrition Labels
and no labels. If there are letter nutrition labels, it encourages
customers to buy healthier groceries while increasing the basket scores
by 3.18 units compared to the control group. (2) Star - Control: There
is a statistically significant difference in basket score between Star
Nutrition Labels and no labels (control). When Star Nutrition Labels are
displayed, they encourage customers to buy healthier groceries,
increasing the basket score by 2.18 units compared to the control group.
(3) Warn - Control: There is a statistically significant difference in
basket score between Warn Nutrition Labels and no labels. Warning Labels
have the strongest effect, leading to a increase of 5.38 units in basket
scores compared to the control group. (4) Warn - Letter: When comparing
Warning Nutrition Labels and Letter Nutrition Labels, there is a
statistically significant difference in basket scores. Warning Labels
are more effective, increasing basket scores by 2.18 units compared to
Letter Nutrition Labels. (5) Warn - Star: When comparing Warning
Nutrition Labels and Star Nutrition Labels, there is a statistically
significant difference in basket scores. Warning Labels are more
effective, increasing basket scores by 3.19 units compared to Star
Nutrition Labels. (6) Star - Letter: It is not statistically significant
in basket scores between Star and Letter labels.</p>
</div>
<div id="d-regression-analysis" class="section level4">
<h4>(d) Regression Analysis</h4>
<pre class="r"><code>### Regression
OLS &lt;- lm(Grocery$BasketScore ~ Grocery$Condition)
summary(OLS)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Grocery$BasketScore ~ Grocery$Condition)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.998  -8.141  -0.286   8.677  44.730 
## 
## Coefficients:
##                         Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)              55.2703     0.5566  99.294  &lt; 2e-16 ***
## Grocery$ConditionLetter   3.1895     0.7872   4.052 5.28e-05 ***
## Grocery$ConditionStar     2.1784     0.7872   2.767   0.0057 ** 
## Grocery$ConditionWarn     5.3753     0.7872   6.828 1.14e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.45 on 1996 degrees of freedom
## Multiple R-squared:  0.02362,    Adjusted R-squared:  0.02215 
## F-statistic: 16.09 on 3 and 1996 DF,  p-value: 2.45e-10</code></pre>
<p>At a 0.05 significance level, the overall model is statistically
significant but R-squared is low. Only 2.3% of the variation in the
dependent variable is explained by independent variables. In terms of
coefficients of the independent variables, all coefficients are
statistically significant at 0.05 significance level. The baseline
Basket Score for the control group is 55.27 units, meaning that the
average basket scores without labels is 55.27 units. In case of Warning
Nutrition Labels, if there is one unit increase in the use of Warning
Nutrition Labels, basket scores increase by 5.37 units on average.</p>
<p><br></p>
</div>
</div>
<div id="additional-analyses" class="section level3">
<h3>4. Additional Analyses</h3>
<div id="a-dunnetts-test" class="section level4">
<h4>(a) Dunnett’s Test</h4>
<p>Dunnett’s Test is specifically designed to compare the control group
with each of the treatment groups individually.</p>
<pre class="r"><code>DunnettTest(x=Grocery$BasketScore, g=Grocery$Condition)</code></pre>
<pre><code>## 
##   Dunnett&#39;s test for comparing several treatments with a control :  
##     95% family-wise confidence level
## 
## $Control
##                    diff    lwr.ci   upr.ci    pval    
## Letter-Control 3.189506 1.3388959 5.040117 0.00016 ***
## Star-Control   2.178433 0.3278227 4.029044 0.01582 *  
## Warn-Control   5.375293 3.5246827 7.225904 2.4e-11 ***
## 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Interpretation: All treatment groups increase the basket scores
compared to the control group which means that all of them are effective
to encourage people to buy healthier food. In case of Warning Nutrition
Labels, it is the most effective labeling method compared to the other
options by increasing 5.37 units of the basket scores.</p>
</div>
<div id="b-binary-analysis-warning-vs.-others" class="section level4">
<h4>(b) Binary Analysis: Warning vs. Others</h4>
<pre class="r"><code># Make Warning Nutrition Labels as 1 and the other labels (Control, Letter, Star) as 0
Grocery$Warning &lt;- 0
Grocery$Warning[Grocery$Condition == &quot;Warn&quot;] &lt;- 1</code></pre>
<pre class="r"><code>OLS2 &lt;- lm(Grocery$BasketScore ~ Grocery$Warning)
summary(OLS2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Grocery$BasketScore ~ Grocery$Warning)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -38.156  -8.304  -0.095   8.857  42.940 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      57.0596     0.3226 176.880  &lt; 2e-16 ***
## Grocery$Warning   3.5860     0.6452   5.558 3.09e-08 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 12.49 on 1998 degrees of freedom
## Multiple R-squared:  0.01523,    Adjusted R-squared:  0.01473 
## F-statistic: 30.89 on 1 and 1998 DF,  p-value: 3.092e-08</code></pre>
<p>When there are not Warning Nutrition Labels, the average basket score
is 57.06. If there is Warning Nutrition Labels, it increases the basket
scores by 3.58 units. At 0.05 significance level, the coefficient of
Warning Labels is statistically significant. The overall model is valid
due to the p-value less than 0.05 significance level.</p>
<p>Compare the mean difference between Warning Nutrition Labels and the
other label options to identify any significant mean difference in
Waring Nutrition Labels in the basket scores compared with the other
options.</p>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Warning}} \leq \mu_{\text{Other}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>: <span
class="math display">\[
H_A: \mu_{\text{Warning}} &gt; \mu_{\text{Other}}
\]</span></p>
<pre class="r"><code>t.test(Grocery$BasketScore[Grocery$Warning==1], Grocery$BasketScore[Grocery$Warning==0], alternative = &quot;greater&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Grocery$BasketScore[Grocery$Warning == 1] and Grocery$BasketScore[Grocery$Warning == 0]
## t = 5.4558, df = 829.19, p-value = 3.22e-08
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  2.503641      Inf
## sample estimates:
## mean of x mean of y 
##  60.64558  57.05960</code></pre>
<p>At 0.05 significance level, the null hypothesis can be rejected.
There is a statistically significant mean difference in the basket
scores between Warning Nutrition Labels and other Label options and the
mean basket scores of Warning Nutrition Labels is greater than the other
labels.</p>
</div>
</div>
<div id="pairwise-comparisons" class="section level3">
<h3>5. Pairwise Comparisons</h3>
<div id="a-warning-vs.-nutrition-star-letter" class="section level4">
<h4>(a) Warning vs. Nutrition (Star + Letter)</h4>
<pre class="r"><code># Combine Letter and Star as Nutrition
Grocery$Condition2 &lt;- Grocery$Condition
Grocery$Condition2[Grocery$Condition == &quot;Letter&quot; | Grocery$Condition == &quot;Star&quot;] &lt;- &quot;Nutrition&quot;

t.test(Grocery$BasketScore[Grocery$Condition2 == &quot;Warn&quot;], 
       Grocery$BasketScore[Grocery$Condition2 == &quot;Nutrition&quot;], 
       alternative = &quot;greater&quot;, 
       var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Grocery$BasketScore[Grocery$Condition2 == &quot;Warn&quot;] and Grocery$BasketScore[Grocery$Condition2 == &quot;Nutrition&quot;]
## t = 3.8696, df = 967.64, p-value = 5.815e-05
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  1.546231      Inf
## sample estimates:
## mean of x mean of y 
##  60.64558  57.95425</code></pre>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Warn}} \leq \mu_{\text{Nutrition}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>: <span
class="math display">\[
H_A: \mu_{\text{Warn}} &gt; \mu_{\text{Nutrition}}
\]</span> At 0.05 significance level, the null hypothesis can be
rejected. The mean basket scores of Warning Nutrition Labels are greater
than the mean basket scores of combined Nutrition (Star and Letter)
Labels.</p>
</div>
<div id="b-nutrition-star-letter-vs.-control" class="section level4">
<h4>(b) Nutrition (Star + Letter) vs. Control</h4>
<pre class="r"><code>t.test(Grocery$BasketScore[Grocery$Condition2==&quot;Nutrition&quot;], 
       Grocery$BasketScore[Grocery$Condition2==&quot;Control&quot;], 
       alternative = &quot;greater&quot;, 
       var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  Grocery$BasketScore[Grocery$Condition2 == &quot;Nutrition&quot;] and Grocery$BasketScore[Grocery$Condition2 == &quot;Control&quot;]
## t = 4.0079, df = 1017.4, p-value = 3.286e-05
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  1.581467      Inf
## sample estimates:
## mean of x mean of y 
##  57.95425  55.27028</code></pre>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Nutrition}} \leq \mu_{\text{Control}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>: <span
class="math display">\[
H_A: \mu_{\text{Nutrition}} &gt; \mu_{\text{Control}}
\]</span> At 0.05 significance level, there is a statistically
significant difference in the basket scores between Nutrition labels
(Star and Letter) and Control groups. The null hypothesis can be
rejected and the mean basket scores of Nutrition labels (Star and
Letter) is greater than the mean basket scores of control groups.</p>
<p><br></p>
</div>
</div>
<div id="conclusion-4" class="section level3">
<h3>6. Conclusion</h3>
<ul>
<li>Warning Labels are the most effective treatment, significantly
increasing basket scores compared to all other treatments and the
Control group.</li>
<li>Nutrition Labels (Star + Letter) are also effective but less
impactful than Warning Labels.</li>
<li>Future recommendations could focus on optimizing the design and
implementation of Warning Labels to maximize their impact on healthier
grocery choices.</li>
</ul>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="parametric-and-non-parametric-regression-analysis"
class="section level2">
<h2>10. Parametric and Non-parametric Regression Analysis</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-8" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Regression.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. To Determine whether a Parametric or Non-parametric fit well with
the data.]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="objective-1" class="section level3">
<h3>2. Objective</h3>
<p>To analyze the relationship between <strong>Price</strong> and
<strong>Unit Sales</strong> using various regression approaches,
including linear, non-linear, and non-parametric models.</p>
<p><br></p>
</div>
<div id="methodology-3" class="section level3">
<h3>3. Methodology</h3>
<div id="a-data-preparation" class="section level4">
<h4>(a) Data Preparation</h4>
<pre class="r"><code>load(&quot;Prices.Rdata&quot;)</code></pre>
</div>
<div id="b-visualizations" class="section level4">
<h4>(b) Visualizations</h4>
<div id="i-boxplot-of-unit-sales-by-price-categorical"
class="section level5">
<h5>(i) Boxplot of Unit Sales by Price (Categorical)</h5>
<pre class="r"><code>### Make a factor variable of prices to convert a continous variable into a categorical variable 
prices$PriceFactor &lt;- as.factor(prices$Price)
### Figures using price as a factor
ggplot(prices, aes(x=PriceFactor, y=UnitSales)) + 
 geom_boxplot()</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
</div>
<div id="ii-scatterplot-of-unit-sales-vs.-price-continuous"
class="section level5">
<h5>(ii) Scatterplot of Unit Sales vs. Price (Continuous)</h5>
<p>Linear Trend:</p>
<pre class="r"><code>ggplot(prices, aes(x=Price, y=UnitSales)) + 
 geom_point() +
 geom_smooth(method=lm)</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-27-1.png" width="672" />
Non-linear Trend (Loess):</p>
<pre class="r"><code>ggplot(prices, aes(x=Price, y=UnitSales)) + 
 geom_point() +
 geom_smooth(method = &quot;loess&quot;)</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-28-1.png" width="672" /></p>
<p><br></p>
</div>
</div>
<div id="c-regression-models" class="section level4">
<h4>(c) Regression Models</h4>
<div id="i-linear-regression-price-as-continuous"
class="section level5">
<h5>(i) Linear Regression (Price as Continuous)</h5>
<pre class="r"><code>### Simple Regression Models
 OLS1 &lt;- lm(prices$UnitSales ~ prices$Price)
 summary(OLS1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = prices$UnitSales ~ prices$Price)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -237.037  -53.092    2.408   52.501  219.927 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1521.353     14.223  106.96   &lt;2e-16 ***
## prices$Price -557.408      9.333  -59.72   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 75.5 on 998 degrees of freedom
## Multiple R-squared:  0.7814, Adjusted R-squared:  0.7811 
## F-statistic:  3567 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Interpretation: The above linear relationship while treating Price as
a continuous variable. The Price coefficient is statistically
significant at 0.05 significance level. When the Price increases by 1
unit, the Unit sales decreases by 557.408 units.</p>
</div>
<div id="ii-linear-regression-price-as-categorical"
class="section level5">
<h5>(ii) Linear Regression (Price as Categorical)</h5>
<pre class="r"><code>OLS2 &lt;- lm(prices$UnitSales ~ prices$PriceFactor)
summary(OLS2)</code></pre>
<pre><code>## 
## Call:
## lm(formula = prices$UnitSales ~ prices$PriceFactor)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -224.580  -47.970    0.937   51.335  200.208 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)             878.752      6.930 126.811  &lt; 2e-16 ***
## prices$PriceFactor1.19  -33.173     10.368  -3.199  0.00142 ** 
## prices$PriceFactor1.29  -65.624      9.800  -6.696 3.57e-11 ***
## prices$PriceFactor1.39 -112.129      9.692 -11.569  &lt; 2e-16 ***
## prices$PriceFactor1.49 -166.827      9.573 -17.427  &lt; 2e-16 ***
## prices$PriceFactor1.59 -223.073      9.800 -22.763  &lt; 2e-16 ***
## prices$PriceFactor1.69 -281.248      9.446 -29.773  &lt; 2e-16 ***
## prices$PriceFactor1.79 -372.416      9.917 -37.553  &lt; 2e-16 ***
## prices$PriceFactor1.89 -442.961      9.573 -46.273  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 72.35 on 991 degrees of freedom
## Multiple R-squared:  0.8007, Adjusted R-squared:  0.7991 
## F-statistic: 497.6 on 8 and 991 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Interpretation: Compared to the linear regression model with the same
coefficient slope making the linearly decreasing regression model, when
making Price variable as a categorical variable, the coefficient
variables vary depending on different price factors, indicating the
relationship between Unit sales and Price factors is non-linear.</p>
</div>
<div id="iii-polynomial-regression" class="section level5">
<h5>(iii) Polynomial Regression</h5>
<pre class="r"><code># Add higher-order terms for polynomial regression
prices$PriceSq &lt;- prices$Price^2
prices$PriceCu &lt;- prices$Price^3

OLS3 &lt;- lm(prices$UnitSales ~ prices$Price + prices$PriceSq + prices$PriceCu)
summary(OLS3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = prices$UnitSales ~ prices$Price + prices$PriceSq + 
##     prices$PriceCu)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -226.879  -49.056   -0.159   51.812  202.625 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)      819.46     571.58   1.434    0.152
## prices$Price     339.71    1185.53   0.287    0.775
## prices$PriceSq  -227.29     806.27  -0.282    0.778
## prices$PriceCu   -32.03     179.96  -0.178    0.859
## 
## Residual standard error: 72.3 on 996 degrees of freedom
## Multiple R-squared:  0.7999, Adjusted R-squared:  0.7993 
## F-statistic:  1327 on 3 and 996 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="iv-log-linear-regression" class="section level5">
<h5>(iv) Log-Linear Regression</h5>
<pre class="r"><code>prices$LnPrice &lt;- log(prices$Price)
OLS4 &lt;- lm(prices$UnitSales ~ prices$LnPrice)
summary(OLS4)</code></pre>
<pre><code>## 
## Call:
## lm(formula = prices$UnitSales ~ prices$LnPrice)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -237.877  -56.347    1.859   54.759  228.611 
## 
## Coefficients:
##                Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     998.438      6.166  161.92   &lt;2e-16 ***
## prices$LnPrice -802.288     14.364  -55.85   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 79.5 on 998 degrees of freedom
## Multiple R-squared:  0.7576, Adjusted R-squared:  0.7574 
## F-statistic:  3119 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
<div id="v-exponential-regression" class="section level5">
<h5>(v) Exponential Regression</h5>
<pre class="r"><code>prices$ExpPrice &lt;- exp(prices$Price)
OLS5 &lt;- lm(prices$UnitSales ~ prices$ExpPrice)
summary(OLS5)</code></pre>
<pre><code>## 
## Call:
## lm(formula = prices$UnitSales ~ prices$ExpPrice)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -229.673  -50.011    0.303   50.166  203.604 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     1255.852      9.372  134.00   &lt;2e-16 ***
## prices$ExpPrice -123.264      1.959  -62.93   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 72.45 on 998 degrees of freedom
## Multiple R-squared:  0.7987, Adjusted R-squared:  0.7985 
## F-statistic:  3960 on 1 and 998 DF,  p-value: &lt; 2.2e-16</code></pre>
</div>
</div>
</div>
<div id="results-comparison" class="section level3">
<h3>4. Results Comparison</h3>
<ul>
<li><strong>Polynomial Regression (OLS3)</strong>: Captures non-linear
patterns with significant coefficients for higher-order terms.</li>
<li><strong>Log-Linear Regression (OLS4)</strong>: Provides a
logarithmic fit, indicating percentage changes in Unit Sales relative to
Price changes.</li>
<li><strong>Exponential Regression (OLS5)</strong>: Models rapid
increases or decreases in Unit Sales.</li>
<li><strong>Non-Parametric (Price as Categorical)</strong>: Achieves the
highest Adjusted R-squared value (0.7791) and the lowest residual
standard error. However, non-parametric models have limited scalability
outside experimental price points.</li>
</ul>
</div>
<div id="conclusion-5" class="section level3">
<h3>5. Conclusion</h3>
<p>Among the models, treating Price as a categorical variable yielded
the best fit (highest Adjusted R-squared). However, for prediction
purposes, polynomial and log-linear regressions are more scalable and
practical for prices beyond the experimental range.</p>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="savings-analysis" class="section level2">
<h2>11. Savings Analysis</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<div id="objective-2" class="section level3">
<h3>1. Objective</h3>
<p>To analyze the impact of treatment on deposit amounts across
different income groups and age categories using t-tests and regression
models. The analysis also explores conditional average treatment effects
(CATEs) to identify individual-level treatment heterogeneity.</p>
<p><br></p>
</div>
<div id="methodology-4" class="section level3">
<h3>2. Methodology</h3>
<div id="a-visualizations" class="section level4">
<h4>(a) Visualizations</h4>
<div id="i-deposits-by-age-and-treatment-group" class="section level5">
<h5>(i) Deposits by Age and Treatment Group</h5>
<pre class="r"><code>load(&quot;Savings.Rdata&quot;)</code></pre>
<pre class="r"><code>ggplot(Savings, aes(x=Age, y=Deposits, color=TreatGroup)) + 
  geom_point() +
  geom_smooth(method=lm, aes(group=TreatGroup))</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-35-1.png" width="672" />
##### (ii) Deposits by Income and Treatment Group</p>
<pre class="r"><code>ggplot(Savings, aes(x=Income, y=Deposits, color=TreatGroup)) + 
  geom_point() +
  geom_smooth(method=lm, aes(group=TreatGroup))</code></pre>
<pre><code>## `geom_smooth()` using formula = &#39;y ~ x&#39;</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-36-1.png" width="672" /></p>
</div>
</div>
<div id="b-group-comparisons-using-t-tests" class="section level4">
<h4>(b) Group Comparisons Using t-tests</h4>
<div id="i-create-income-groups-and-age-dummies" class="section level5">
<h5>(i) Create Income Groups and Age Dummies</h5>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>:</p>
<p><span class="math display">\[
H_0: \mu_{\text{Treatment}} \leq \mu_{\text{Control}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>:</p>
<p><span class="math display">\[
H_A: \mu_{\text{Treatment}} &gt; \mu_{\text{Control}}
\]</span></p>
<pre class="r"><code>### t-tests by group
### Income group and age dummies
Savings$Age35 &lt;- ifelse(Savings$Age&gt;=35, 1, 0)
Savings$IncGrp &lt;- &quot;&lt;$50K&quot;
Savings$IncGrp &lt;- ifelse(Savings$Income&gt;=50000, &quot;$50-100K&quot;, Savings$IncGrp)
Savings$IncGrp &lt;- ifelse(Savings$Income&gt;100000, &quot;&gt;$100K&quot;, Savings$IncGrp)

### t-tests
INC1t &lt;- subset(Savings$Deposits, Savings$TreatGroup == 1 &amp;     Savings$IncGrp==&quot;&lt;$50K&quot;)

INC1c &lt;- subset(Savings$Deposits, Savings$TreatGroup == 0 &amp;  Savings$IncGrp==&quot;&lt;$50K&quot;)

INC2t &lt;- subset(Savings$Deposits, Savings$TreatGroup == 1 &amp; Savings$IncGrp==&quot;$50-100K&quot;)

INC2c &lt;- subset(Savings$Deposits, Savings$TreatGroup == 0 &amp; Savings$IncGrp==&quot;$50-100K&quot;)

INC3t &lt;- subset(Savings$Deposits, Savings$TreatGroup == 1 &amp; Savings$IncGrp==&quot;&gt;$100K&quot;)

INC3c &lt;- subset(Savings$Deposits, Savings$TreatGroup == 0 &amp; Savings$IncGrp==&quot;&gt;$100K&quot;)

INCt &lt;- subset(Savings$Deposits, Savings$TreatGroup == 1)
INCc &lt;- subset(Savings$Deposits, Savings$TreatGroup == 0)

### Entire population
t.test(INCt, INCc, alternative = &quot;greater&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  INCt and INCc
## t = 14.724, df = 4950.6, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  476.9293      Inf
## sample estimates:
## mean of x mean of y 
##  1918.569  1381.646</code></pre>
<p>Compare the deposit amounts between control group and treatment group
to identify if there is any difference in mean deposit amounts between
the two groups. At 0.05 significance level, the null hypothesis can be
rejected and the mean deposit amount of the treatment group is greater
than the mean deposit amount of the control group.</p>
</div>
<div id="ii-perform-t-tests-for-each-income-group"
class="section level5">
<h5>(ii) Perform t-tests for Each Income Group</h5>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Treatment, Income Group 1}} \leq \mu_{\text{Control,
Income Group 1}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>: <span
class="math display">\[
H_A: \mu_{\text{Treatment, Income Group 1}} &gt; \mu_{\text{Control,
Income Group 1}}
\]</span></p>
<pre class="r"><code>### Income group 1 (&lt;$50K)
t.test(INC1t, INC1c, alternative = &quot;greater&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  INC1t and INC1c
## t = 11.469, df = 2831.4, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  468.0417      Inf
## sample estimates:
## mean of x mean of y 
##  1794.196  1247.762</code></pre>
<p>In the income group 1 (&lt;$50K), the null hypothesis can be rejected
at 0.05 significance level, the mean deposit amount of the treatment
group in the income group 1 is greater than the mean deposit amount of
the control group in the income group 1.</p>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Treatment, Income Group 2}} \leq \mu_{\text{Control,
Income Group 2}}
\]</span></p>
<p><strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>: <span
class="math display">\[
H_A: \mu_{\text{Treatment, Income Group 2}} &gt; \mu_{\text{Control,
Income Group 2}}
\]</span></p>
<pre class="r"><code>### Income group 2 ($50-100K)
t.test(INC2t, INC2c, alternative = &quot;greater&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  INC2t and INC2c
## t = 9.6702, df = 1750.4, p-value &lt; 2.2e-16
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  487.889     Inf
## sample estimates:
## mean of x mean of y 
##  2057.428  1469.479</code></pre>
<p>In the income group 2 ($50K-$100K), the null hypothesis can be
rejected at 0.05 significance level, the mean deposit amount of the
treatment group in the income group 2 is greater than the mean deposit
amount of the control group in the income group 2.</p>
<p><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>)</strong>: <span
class="math display">\[
H_0: \mu_{\text{Treatment, Income Group 3}} \leq \mu_{\text{Control,
Income Group 3}}
\]</span> <strong>Alternative Hypothesis (<span
class="math inline">\(H_A\)</span>)</strong>:</p>
<p><span class="math display">\[
H_A: \mu_{\text{Treatment, Income Group 3}} &gt; \mu_{\text{Control,
Income Group 3}}
\]</span></p>
<pre class="r"><code>### Income group 3 (&gt;$100K)
t.test(INC3t, INC3c, alternative = &quot;greater&quot;, var.equal = FALSE)</code></pre>
<pre><code>## 
##  Welch Two Sample t-test
## 
## data:  INC3t and INC3c
## t = 1.1251, df = 354.7, p-value = 0.1307
## alternative hypothesis: true difference in means is greater than 0
## 95 percent confidence interval:
##  -70.0936      Inf
## sample estimates:
## mean of x mean of y 
##  2215.043  2064.558</code></pre>
<p>In the income group 3 (&gt;$100K), the null hypothesis can be
rejected at 0.05 significance level, the mean deposit amount of the
treatment group in the income group 3 is greater than the mean deposit
amount of the control group in the income group 3.</p>
<p><br></p>
</div>
</div>
</div>
<div id="regression-analysis" class="section level3">
<h3>3. Regression Analysis</h3>
<div id="a-model-1-income-group-treatment-effects"
class="section level4">
<h4>(a) Model 1: Income Group Treatment Effects</h4>
<pre class="r"><code>### A regression approach
### Create Variables
Savings$INC1&lt;- ifelse(Savings$IncGrp==&quot;&lt;$50K&quot;,1,0)
Savings$INC2 &lt;- ifelse(Savings$IncGrp==&quot;$50-100K&quot;,1,0)
Savings$INC3 &lt;- ifelse(Savings$IncGrp==&quot;&gt;$100K&quot;,1,0)
Savings$TreatInc1 &lt;- Savings$TreatGroup * Savings$INC1
Savings$TreatInc2 &lt;- Savings$TreatGroup * Savings$INC2
Savings$TreatInc3 &lt;- Savings$TreatGroup * Savings$INC3
### First Model
OLS&lt;-lm(Savings$Deposits ~ Savings$TreatInc1 + Savings$TreatInc2 + Savings$TreatInc3 + Savings$INC1 + Savings$INC2 + Savings$INC3)
summary(OLS)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Savings$Deposits ~ Savings$TreatInc1 + Savings$TreatInc2 + 
##     Savings$TreatInc3 + Savings$INC1 + Savings$INC2 + Savings$INC3)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2929.2  -945.9     8.7   900.2  3336.3 
## 
## Coefficients: (1 not defined because of singularities)
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)        2064.56      97.77  21.116  &lt; 2e-16 ***
## Savings$TreatInc1   546.43      47.62  11.476  &lt; 2e-16 ***
## Savings$TreatInc2   587.95      60.58   9.705  &lt; 2e-16 ***
## Savings$TreatInc3   150.48     135.09   1.114    0.265    
## Savings$INC1       -816.80     103.26  -7.910 3.15e-15 ***
## Savings$INC2       -595.08     106.40  -5.593 2.35e-08 ***
## Savings$INC3            NA         NA      NA       NA    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1275 on 4994 degrees of freedom
## Multiple R-squared:  0.06102,    Adjusted R-squared:  0.06008 
## F-statistic: 64.91 on 5 and 4994 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation</strong>: - The intercept represents deposits
for the control group in Income Group 3 (&gt;$100K): 2064.56 units. -
<strong>Treatment Effects</strong>: - Income Group 1: +546.43 units
(statistically significant). - Income Group 2: +587.95 units
(statistically significant). - Income Group 3: +150.84 units (not
statistically significant). - Income groups except for Income group 3
has the lower savings amount compared to the Income group 3. - The
treatment is the difference in the savings amount between the control
group and the treatment group for each income group.</p>
</div>
<div id="b-model-2-interaction-of-treatment-and-income-groups"
class="section level4">
<h4>(b) Model 2: Interaction of Treatment and Income Groups</h4>
<pre class="r"><code>OLS &lt;- lm(Savings$Deposits ~ Savings$TreatGroup * Savings$IncGrp)
summary(OLS)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Savings$Deposits ~ Savings$TreatGroup * Savings$IncGrp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2929.2  -945.9     8.7   900.2  3336.3 
## 
## Coefficients:
##                                         Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                              1469.48      41.98  35.002  &lt; 2e-16
## Savings$TreatGroup                        587.95      60.58   9.705  &lt; 2e-16
## Savings$IncGrp&lt;$50K                      -221.72      53.54  -4.141 3.51e-05
## Savings$IncGrp&gt;$100K                      595.08     106.40   5.593 2.35e-08
## Savings$TreatGroup:Savings$IncGrp&lt;$50K    -41.52      77.05  -0.539  0.59004
## Savings$TreatGroup:Savings$IncGrp&gt;$100K  -437.46     148.05  -2.955  0.00314
##                                            
## (Intercept)                             ***
## Savings$TreatGroup                      ***
## Savings$IncGrp&lt;$50K                     ***
## Savings$IncGrp&gt;$100K                    ***
## Savings$TreatGroup:Savings$IncGrp&lt;$50K     
## Savings$TreatGroup:Savings$IncGrp&gt;$100K ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1275 on 4994 degrees of freedom
## Multiple R-squared:  0.06102,    Adjusted R-squared:  0.06008 
## F-statistic: 64.91 on 5 and 4994 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation</strong>: In the first regression model, it
represents the treatment effect for each income group. The first
regression shows different income levels affect the amount of deposits
without any treatment effect. However, the most important thing to
discuss is that it represents the treatment effect for each income
group. Therefore, this regression model shows the treatment effect for
each income level comparing with the control group for each income
group. In contrast, the second regression model shows the baseline
treatment effect of the income group 2 ($50-100K) which allows us to
compare the treatment effect among different income levels. First, the
income group 2 received the treatment effect increases deposit amounts
by 587.95 units on average when comparing the income group 2 within the
control group. The income group 1 received the treatment effect saved
41.52 units less than the income group 2 received the treatment effect.
However, this estimate is not statistically significant, meaning that
this difference is not a meaningful difference but just random
variation. However, the income group 3 in the treatment effect saved
437.46 units less than the income 2 group under the treatment effect and
this estimate is statistically significant,concluding that the treatment
effect differs between these groups.</p>
</div>
<div id="c-model-3-including-age-interactions" class="section level4">
<h4>(c) Model 3: Including Age Interactions</h4>
<pre class="r"><code>### Third Model, with Age
OLS&lt;-lm(Savings$Deposits ~ Savings$TreatGroup*Savings$IncGrp + Savings$TreatGroup*Savings$Age35)
summary(OLS)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Savings$Deposits ~ Savings$TreatGroup * Savings$IncGrp + 
##     Savings$TreatGroup * Savings$Age35)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2725.22  -913.01   -17.04   902.40  2853.08 
## 
## Coefficients:
##                                         Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                               803.44      48.81  16.461  &lt; 2e-16
## Savings$TreatGroup                        352.64      70.26   5.019 5.36e-07
## Savings$IncGrp&lt;$50K                      -206.15      47.90  -4.303 1.71e-05
## Savings$IncGrp&gt;$100K                      583.82      95.19   6.133 9.29e-10
## Savings$Age35                            1010.01      47.27  21.367  &lt; 2e-16
## Savings$TreatGroup:Savings$IncGrp&lt;$50K    -38.99      68.94  -0.566 0.571681
## Savings$TreatGroup:Savings$IncGrp&gt;$100K  -460.18     132.46  -3.474 0.000517
## Savings$TreatGroup:Savings$Age35          356.44      67.79   5.258 1.52e-07
##                                            
## (Intercept)                             ***
## Savings$TreatGroup                      ***
## Savings$IncGrp&lt;$50K                     ***
## Savings$IncGrp&gt;$100K                    ***
## Savings$Age35                           ***
## Savings$TreatGroup:Savings$IncGrp&lt;$50K     
## Savings$TreatGroup:Savings$IncGrp&gt;$100K ***
## Savings$TreatGroup:Savings$Age35        ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1140 on 4992 degrees of freedom
## Multiple R-squared:  0.2488, Adjusted R-squared:  0.2477 
## F-statistic: 236.1 on 7 and 4992 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation</strong>: Here in this regression model, Age
and income are covariates to estimate CATEs (Conditional Average
Treatment effects) which can estimate an individual’s heterogeneous
treatment effect. Age is a categorical variable with two different
value, one of which is younger than 35 (&lt;35) and the other one is
older than or equal to 35 (&gt;=35). Intercept 803.44 units represents
people’s average saving amounts in the control group under 35 (&lt;35)
within the income group 2 ($50-100K). The treatment group effect is
352.64 units, indicating that people received treatment effect who are
younger than 35 within the income group 2 saved more deposits (treatment
effect) on average compared to the control group. We can answer the
following questions by combining the effects of covariates such as age
and income and their corresponding interaction terms with the treatment
group and calculate CATE (Conditional Average Treatment Effect).</p>
<ol style="list-style-type: decimal">
<li>What would the treatment effect be for some of age 30 with income of
$40K? The treatment effect is 38.99 units less than the control group
with the same age and within the same income group , which is people
younger than 30 within the income group 1 in the control group, which is
803.44 - 206.15 = 597.29 units. In conclusion, the treatment effect is
558.30 units.</li>
<li>What would the treatment effect be for some of age 30 with income of
$75K? The treatment effect is 325.64 units greater than the control
group with the same age and within the same income group, which is
people younger than 30 within income group2 in the control group, which
is 803.44. In conclusion, the treatment effect is 1129.08 units.</li>
</ol>
</div>
<div id="d-model-4-continuous-income-and-age" class="section level4">
<h4>(d) Model 4: Continuous Income and Age</h4>
<pre class="r"><code>### Fourth model, with continuous interactions
# Appropriate if the treatment effect relationship is linear
OLS&lt;-lm(Savings$Deposits ~ Savings$TreatGroup*Savings$Income + Savings$TreatGroup*Savings$Age)
summary(OLS)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Savings$Deposits ~ Savings$TreatGroup * Savings$Income + 
##     Savings$TreatGroup * Savings$Age)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1988.6  -905.9   -10.7   924.4  2062.0 
## 
## Coefficients:
##                                     Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                       -1.026e+03  7.630e+01 -13.450  &lt; 2e-16 ***
## Savings$TreatGroup                 3.670e+02  1.075e+02   3.413 0.000647 ***
## Savings$Income                     7.807e-03  6.884e-04  11.340  &lt; 2e-16 ***
## Savings$Age                        4.847e+01  1.545e+00  31.367  &lt; 2e-16 ***
## Savings$TreatGroup:Savings$Income -4.383e-03  9.749e-04  -4.496 7.10e-06 ***
## Savings$TreatGroup:Savings$Age     8.963e+00  2.186e+00   4.101 4.19e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1049 on 4994 degrees of freedom
## Multiple R-squared:  0.3637, Adjusted R-squared:  0.363 
## F-statistic: 570.9 on 5 and 4994 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation</strong>: In this regression model, covariates
such as income and age are both treated an continuous. In case of the
intercept, it represents people in the control group when their age and
income are both zero saved -1026 units on average but it is not
realistic because in the real world, age and income cannot be zero. When
people in the control group with their age and income being zero are
treated, their savings increases by 367 units on average. When income
increases by 1 unit in the control group, the savings increases by
0.007807 units on average. Similarly, if age increases by 1 unit, the
savings increases by 48.47 units on average. When income increases by 1
unit , the treatment effect decreases by 0.004383 units on average
whereas when age increases by 1 unit, the treatment effect increases by
8.963 units on average.</p>
</div>
</div>
<div id="conclusion-6" class="section level3">
<h3>4. Conclusion</h3>
<ul>
<li>The first model focuses on treatment effects for each income group,
showing significant increases in deposits for Income Groups 1 and
2.</li>
<li>The second model emphasizes comparisons between groups, revealing
significant differences between Income Groups 3 and 2 under
treatment.</li>
<li>The inclusion of age in the third and fourth models highlights the
role of individual-level covariates in explaining treatment
heterogeneity.</li>
<li>Continuous interaction models provide nuanced insights but require
careful interpretation due to extrapolation limitations.</li>
</ul>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="difference-in-differences" class="section level2">
<h2>12. Difference in Differences</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-9" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Expedia.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Does responding to negative reviews lead to more negative review?]
</p>
<p><br></p>
<p><br><br></p>
</div>
<div id="objective-3" class="section level3">
<h3>2. Objective</h3>
<p>There are two groups (Expedia.com and Hotels.com) and control period
is from Jan to June whereas the treatment period is from July - Dec. At
Expedia.com, the study aims to assess whether responding to negative
reviews leads to an increase in negative reviews from July to Dec. In
order to identify the true causal effect, we set Hotels.com as a control
group and examine what would have happened to the number of negative
reviews if staff had not responded to negative reviews by comparing
negative reviews between Expedia.com and Hotels.com during the control
period and the treatment period to determine whether responses to
negative reviews increase the number of negative reviews.</p>
<p><br></p>
</div>
<div id="hypotheses-2" class="section level3">
<h3>3. Hypotheses</h3>
<p><strong>Null Hypothesis (H₀):</strong></p>
<p>Responding to negative reviews does not increase the number of
negative reviews on Expedia.com: <span class="math display">\[
H_0: \Delta \text{NegativeReviews}_{Expedia} - \Delta
\text{NegativeReviews}_{Hotels} \leq 0
\]</span></p>
<p><strong>Alternative Hypothesis (H₁):</strong></p>
<p>Responding to negative reviews increases the number of negative
reviews on Expedia.com: <span class="math display">\[
H_1: \Delta \text{NegativeReviews}_{Expedia} - \Delta
\text{NegativeReviews}_{Hotels} &gt; 0
\]</span></p>
<p><br></p>
</div>
<div id="methodology-summary-2" class="section level3">
<h3>4. Methodology &amp; Summary</h3>
<div id="a-data-preparation-1" class="section level4">
<h4>(a) Data Preparation</h4>
<pre class="r"><code>library(dplyr)</code></pre>
<pre><code>## 
## Attaching package: &#39;dplyr&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     filter, lag</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     intersect, setdiff, setequal, union</code></pre>
<pre class="r"><code>library(ggplot2)

setwd(&#39;C:/Users/sjh50/OneDrive/문서/UC Davis/Paul/Github/Lecture3&#39;)
### Load data :: This data will include data frames for all three examples
load(&quot;DiD.Rdata&quot;)

### Create Treatment Period and Treatment Group variable
### The treatment period is from July to Dec at Expedia.com
Hotels$TreatPer = Hotels$Month&gt;6
Hotels$TreatGroup = Hotels$Website==&quot;Expedia&quot;

### Summarize Y by Group
summary_table &lt;- Hotels %&gt;%
  group_by(TreatGroup,TreatPer) %&gt;%
  summarize(NReviews = mean(NegativeReviews, na.rm = TRUE), .groups = &quot;drop&quot;)
print(summary_table)</code></pre>
<pre><code>## # A tibble: 4 × 3
##   TreatGroup TreatPer NReviews
##   &lt;lgl&gt;      &lt;lgl&gt;       &lt;dbl&gt;
## 1 FALSE      FALSE        8.74
## 2 FALSE      TRUE         8.00
## 3 TRUE       FALSE        6.61
## 4 TRUE       TRUE         7.83</code></pre>
</div>
<div id="b-results" class="section level4">
<h4>(b) Results</h4>
<p>Control Group (Hotels.com) - The average number of reviews during
control period (Jan - June) is 8.73 units whereas the average number of
reviews during treatment period (July - Dec) is 8.00 units. The average
number of reviews decreases without any treatment effect.</p>
<p>Treatment Group (Expedia.com) - The average number of review during
control period (Jan - June) is 6.61 units whereas the average number of
reviews during treatment period (July - Dec) is 7.83 units. The average
number of reviews increases with treatment effect.</p>
</div>
<div id="c-difference-in-differences-did" class="section level4">
<h4>(c) Difference in Differences (DiD)</h4>
<pre class="r"><code># Assuming your data frame is named &#39;Hotels&#39; with columns TreatGroup (TRUE/FALSE),
# TreatPer (TRUE/FALSE), and NegativeReviews (numerical)

# Summarizing the data by group and period
library(dplyr)
Hotels &lt;- Hotels %&gt;%
  mutate(
    TreatPer = Month &gt;= 7,  # July to December is the treatment period
    TreatGroup = Website == &quot;Expedia&quot;  # Expedia is the treatment group
  )

summary_table &lt;- Hotels %&gt;%
  group_by(TreatGroup, TreatPer) %&gt;%
  summarize(NReviews = mean(NegativeReviews, na.rm = TRUE), .groups = &quot;drop&quot;)

print(summary_table)</code></pre>
<pre><code>## # A tibble: 4 × 3
##   TreatGroup TreatPer NReviews
##   &lt;lgl&gt;      &lt;lgl&gt;       &lt;dbl&gt;
## 1 FALSE      FALSE        8.74
## 2 FALSE      TRUE         8.00
## 3 TRUE       FALSE        6.61
## 4 TRUE       TRUE         7.83</code></pre>
<pre class="r"><code># Calculating Differences
# Extracting values from the summary table
control_pre &lt;- summary_table %&gt;%
  filter(TreatGroup == FALSE &amp; TreatPer == FALSE) %&gt;%
  pull(NReviews)

control_post &lt;- summary_table %&gt;%
  filter(TreatGroup == FALSE &amp; TreatPer == TRUE) %&gt;%
  pull(NReviews)

treat_pre &lt;- summary_table %&gt;%
  filter(TreatGroup == TRUE &amp; TreatPer == FALSE) %&gt;%
  pull(NReviews)

treat_post &lt;- summary_table %&gt;%
  filter(TreatGroup == TRUE &amp; TreatPer == TRUE) %&gt;%
  pull(NReviews)

# Calculating the differences
diff_control &lt;- control_post - control_pre
diff_treat &lt;- treat_post - treat_pre

# Difference-in-Differences
DiD &lt;- diff_treat - diff_control

# Printing the results
cat(&quot;Control Group Difference (Post - Pre):&quot;, diff_control, &quot;\n&quot;)</code></pre>
<pre><code>## Control Group Difference (Post - Pre): -0.7333333</code></pre>
<pre class="r"><code>cat(&quot;Treatment Group Difference (Post - Pre):&quot;, diff_treat, &quot;\n&quot;)</code></pre>
<pre><code>## Treatment Group Difference (Post - Pre): 1.22</code></pre>
<pre class="r"><code>cat(&quot;Difference-in-Differences (DiD):&quot;, DiD, &quot;\n&quot;)</code></pre>
<pre><code>## Difference-in-Differences (DiD): 1.953333</code></pre>
<p><strong>Interpretation:</strong> 0.733 indicates what would have
happened to the average number of negative reviews when staff had not
responded to negative reviews. 1.22 indicates an increase in the average
number of negative reviews when staff responses to negative reviews. The
difference in difference in the average number of negative reviews
during the treatment period between the control group and the treatment
group is 1.95, indicating responding to the negative reviews increases
the average negative reviews.</p>
</div>
<div id="d-parallel-trends-assumption" class="section level4">
<h4>(d) Parallel Trends Assumption</h4>
<pre class="r"><code>###### Parallel Trends ######
# Raw Data
ggplot(Hotels, aes(x = Month, y = NegativeReviews, color = Website)) +
  stat_summary(fun = mean, geom = &quot;line&quot;) +
  scale_x_continuous(breaks = seq(min(Hotels$Month), max(Hotels$Month), by = 1))</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-46-1.png" width="672" /></p>
<p><strong>Interpretation:</strong> Before the treatment effect (Jan -
June), parallel trends assumption holds during the control period. This
parallel trends assumption is validated when the differences between the
control group and the treatment group during the control period seem
consistent and both follows similar patterns. If this parallel trends
assumption hold, the difference between the treatment group and the
control group during the treatment period is likely to be a true
treatment effect. Therefore, we can conclude that Difference in
Difference in the average number of negative reviews between the control
group and the treatment group during the treatment period is likely
attributable to the true treatment effect which is responding to
negative reviews (treatment effects) at Expedia.</p>
<p><br></p>
</div>
<div id="e-model-estimation" class="section level4">
<h4>(e) Model Estimation</h4>
<pre class="r"><code>### Statistical Test for Parallel Trends Assumption
### The dependent variable Y is the number of negative reviews.
### Categorical variables represent each month to make them as fixed effects   
PTrends &lt;- lm(Hotels$NegativeReviews ~ as.factor(Hotels$Month)*Hotels$TreatGroup)
summary(PTrends)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Hotels$NegativeReviews ~ as.factor(Hotels$Month) * 
##     Hotels$TreatGroup)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
##  -5.00  -1.40  -0.10   1.38   7.14 
## 
## Coefficients:
##                                                   Estimate Std. Error t value
## (Intercept)                                      7.360e+00  3.058e-01  24.065
## as.factor(Hotels$Month)2                        -5.000e-01  4.325e-01  -1.156
## as.factor(Hotels$Month)3                         6.600e-01  4.325e-01   1.526
## as.factor(Hotels$Month)4                         1.640e+00  4.325e-01   3.792
## as.factor(Hotels$Month)5                         2.840e+00  4.325e-01   6.566
## as.factor(Hotels$Month)6                         3.620e+00  4.325e-01   8.370
## as.factor(Hotels$Month)7                         2.040e+00  4.325e-01   4.717
## as.factor(Hotels$Month)8                         1.880e+00  4.325e-01   4.347
## as.factor(Hotels$Month)9                         9.200e-01  4.325e-01   2.127
## as.factor(Hotels$Month)10                       -3.600e-01  4.325e-01  -0.832
## as.factor(Hotels$Month)11                       -4.600e-01  4.325e-01  -1.064
## as.factor(Hotels$Month)12                       -1.600e-01  4.325e-01  -0.370
## Hotels$TreatGroupTRUE                           -2.000e+00  4.325e-01  -4.624
## as.factor(Hotels$Month)2:Hotels$TreatGroupTRUE  -2.200e-01  6.117e-01  -0.360
## as.factor(Hotels$Month)3:Hotels$TreatGroupTRUE  -4.000e-01  6.117e-01  -0.654
## as.factor(Hotels$Month)4:Hotels$TreatGroupTRUE  -6.634e-14  6.117e-01   0.000
## as.factor(Hotels$Month)5:Hotels$TreatGroupTRUE  -1.000e-01  6.117e-01  -0.163
## as.factor(Hotels$Month)6:Hotels$TreatGroupTRUE  -2.000e-02  6.117e-01  -0.033
## as.factor(Hotels$Month)7:Hotels$TreatGroupTRUE   2.680e+00  6.117e-01   4.381
## as.factor(Hotels$Month)8:Hotels$TreatGroupTRUE   1.740e+00  6.117e-01   2.845
## as.factor(Hotels$Month)9:Hotels$TreatGroupTRUE   1.740e+00  6.117e-01   2.845
## as.factor(Hotels$Month)10:Hotels$TreatGroupTRUE  1.920e+00  6.117e-01   3.139
## as.factor(Hotels$Month)11:Hotels$TreatGroupTRUE  1.500e+00  6.117e-01   2.452
## as.factor(Hotels$Month)12:Hotels$TreatGroupTRUE  1.400e+00  6.117e-01   2.289
##                                                 Pr(&gt;|t|)    
## (Intercept)                                      &lt; 2e-16 ***
## as.factor(Hotels$Month)2                        0.247901    
## as.factor(Hotels$Month)3                        0.127286    
## as.factor(Hotels$Month)4                        0.000157 ***
## as.factor(Hotels$Month)5                        7.72e-11 ***
## as.factor(Hotels$Month)6                         &lt; 2e-16 ***
## as.factor(Hotels$Month)7                        2.68e-06 ***
## as.factor(Hotels$Month)8                        1.50e-05 ***
## as.factor(Hotels$Month)9                        0.033619 *  
## as.factor(Hotels$Month)10                       0.405382    
## as.factor(Hotels$Month)11                       0.287749    
## as.factor(Hotels$Month)12                       0.711500    
## Hotels$TreatGroupTRUE                           4.18e-06 ***
## as.factor(Hotels$Month)2:Hotels$TreatGroupTRUE  0.719155    
## as.factor(Hotels$Month)3:Hotels$TreatGroupTRUE  0.513270    
## as.factor(Hotels$Month)4:Hotels$TreatGroupTRUE  1.000000    
## as.factor(Hotels$Month)5:Hotels$TreatGroupTRUE  0.870162    
## as.factor(Hotels$Month)6:Hotels$TreatGroupTRUE  0.973921    
## as.factor(Hotels$Month)7:Hotels$TreatGroupTRUE  1.28e-05 ***
## as.factor(Hotels$Month)8:Hotels$TreatGroupTRUE  0.004522 ** 
## as.factor(Hotels$Month)9:Hotels$TreatGroupTRUE  0.004522 ** 
## as.factor(Hotels$Month)10:Hotels$TreatGroupTRUE 0.001738 ** 
## as.factor(Hotels$Month)11:Hotels$TreatGroupTRUE 0.014338 *  
## as.factor(Hotels$Month)12:Hotels$TreatGroupTRUE 0.022265 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.163 on 1176 degrees of freedom
## Multiple R-squared:  0.3479, Adjusted R-squared:  0.3352 
## F-statistic: 27.28 on 23 and 1176 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation:</strong> If the treatment period starts from
July to Dec, the parallel trends assumption must be validated during the
control period (Jan - June), meaning that the difference in the average
number of negative reviews between the control group and the treatment
group in a specific period compared to the baseline month (Jan) must not
be statistically significant. In contrast, if there is any true
treatment effect during the treatment period, the difference in the
number of negative reviews in a specific month within the treatment
period compared to the difference between the two groups in the baseline
month (Jan) must be statistically significant. At 0.05 significance
level, treatment effects during the control period except for Jan are
not statistically significant whereas the differences between the
control group and the treatment group in the average number of negative
reviews compared to that of the baseline month (Jan) are statistically
significant, indicating those are true treatment effects.</p>
<pre class="r"><code>###### Dif-in-Dif ######
DID1 &lt;- lm(Hotels$NegativeReviews ~ Hotels$TreatPer*Hotels$TreatGroup)
summary(DID1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Hotels$NegativeReviews ~ Hotels$TreatPer * Hotels$TreatGroup)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -6.7367 -1.8333 -0.0033  1.9967  8.3867 
## 
## Coefficients:
##                                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                                 8.7367     0.1468  59.500  &lt; 2e-16
## Hotels$TreatPerTRUE                        -0.7333     0.2077  -3.531 0.000429
## Hotels$TreatGroupTRUE                      -2.1233     0.2077 -10.225  &lt; 2e-16
## Hotels$TreatPerTRUE:Hotels$TreatGroupTRUE   1.9533     0.2937   6.651 4.41e-11
##                                              
## (Intercept)                               ***
## Hotels$TreatPerTRUE                       ***
## Hotels$TreatGroupTRUE                     ***
## Hotels$TreatPerTRUE:Hotels$TreatGroupTRUE ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.543 on 1196 degrees of freedom
## Multiple R-squared:  0.0828, Adjusted R-squared:  0.0805 
## F-statistic: 35.99 on 3 and 1196 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation:</strong> The overall model validity is
validated at 0.05 significance level. All coefficients are statistically
significant at 0.05 significance level.</p>
<p><br></p>
</div>
</div>
<div id="conclusion-7" class="section level3">
<h3>5. Conclusion</h3>
<p><span class="math display">\[
Y = 8.7367 + (-0.7333) \cdot I[\text{TreatPer}] + (-2.1233) \cdot
I[\text{TreatGroup}] + 1.9533 \cdot I[\text{TreatPer}] \times
I[\text{TreatGroup}]
\]</span></p>
<p><strong>Interpretation:</strong> (1) Intercept 8.7367 indicates the
number of negative reviews at Hotels.com (control group). (2) The number
of negative reviews decreases by 0.7333 during the treatment period in
the control group. This figure indicates what would have happened when
staff had not responded to negative reviews during the treatment period
to compare the number of negative reviews when responding to negative
reviews at Expedia.com (3) The number of negative reviews decreases by
2.1233 during the control period in the treatment group (Expedia.com),
meaning that the number of negative reviews at Expedia.com is 2.1233
units less than that of Hotels.com3 (4) 1.9533 is DiD (Difference in
Differences). This is the true treatment effect on the number of
negative reviews when responding to the negative reviews in the
treatment group (Expedia.com) compared to the time effect in the control
group not responding to the negative reviews.</p>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="difference-in-differences-1" class="section level2">
<h2>13. Difference in Differences</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-10" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Gami.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. On-Time Payments: Gamification Impact on App vs. Bank Users]
</p>
<p><br></p>
</div>
<div id="goal-8" class="section level3">
<h3>2. Goal</h3>
<p>To determine whether introducing games (raffles and prize wheels) to
a financial services app encourages on-time bill payment.</p>
<p><br></p>
</div>
<div id="hypotheses-3" class="section level3">
<h3>3. Hypotheses</h3>
<p><strong>Null Hypothesis (H₀):</strong></p>
<p>Introducing games does not affect on-time bill payment: <span
class="math display">\[
H_0: \Delta \text{OnTimePayments}_{AppUsers} - \Delta
\text{OnTimePayments}_{BankUsers} \geq 0
\]</span></p>
<p><strong>Alternative Hypothesis (H₁):</strong></p>
<p>Introducing games reduces on-time bill payment: <span
class="math display">\[
H_1: \Delta \text{OnTimePayments}_{AppUsers} - \Delta
\text{OnTimePayments}_{BankUsers} &lt; 0
\]</span></p>
<p><br></p>
</div>
<div id="methodology-summary-3" class="section level3">
<h3>4. Methodology &amp; Summary</h3>
<div id="a-data-preparation-2" class="section level4">
<h4>(a) Data Preparation</h4>
<pre class="r"><code>library(dplyr)
library(ggplot2)
### Load data :: This data will include data frames for all three examples 
load(&quot;DiD.Rdata&quot;)
### Create time variable 
Gamification$RunMonth &lt;- 1
Gamification$RunMonth[Gamification$Month==11] &lt;- 2
Gamification$RunMonth[Gamification$Month==12] &lt;- 3
Gamification$RunMonth[Gamification$Month==1] &lt;- 4
Gamification$RunMonth[Gamification$Month==2] &lt;- 5
Gamification$RunMonth[Gamification$Month==3] &lt;- 6
### Create Treatment Period and Treatment Group variable 
Gamification$TreatPer = Gamification$RunMonth &gt; 4
Gamification$TreatGroup = Gamification$&#39;AppUser?&#39; == &#39;Yes&#39;
### Summarize Y by Group 
summary_table &lt;- Gamification %&gt;%
  group_by(TreatGroup, TreatPer) %&gt;%
  summarize(OnTime = mean(`OnTime?`, na.rm = TRUE), .groups = &#39;drop&#39;)
print(summary_table)</code></pre>
<pre><code>## # A tibble: 4 × 3
##   TreatGroup TreatPer OnTime
##   &lt;lgl&gt;      &lt;lgl&gt;     &lt;dbl&gt;
## 1 FALSE      FALSE     0.749
## 2 FALSE      TRUE      0.756
## 3 TRUE       FALSE     0.901
## 4 TRUE       TRUE      0.747</code></pre>
</div>
<div id="b-results-1" class="section level4">
<h4>(b) Results</h4>
<p><strong>Interpretation:</strong> The average proportion of on-time
bill payment among bank users not on the application during the control
period is 0.748875, which increases by 0.006875 during the treatment
period in the control group. In contrast, the average proportion of
on-time bill payment among app users during the control period 0.901250
whereas this figure decreases by 0.1545 on average among app users in
the treatment period when app games are available to use. It appears
that introducing new games to app users has a negative impact on the
average proportion of on-time bill payments. This suggests a potential
distraction caused by the availability of games.</p>
</div>
<div id="c-parallel-trends-assumption" class="section level4">
<h4>(c) Parallel Trends Assumption</h4>
<pre class="r"><code>### Parallel Trends 
ggplot(Gamification, aes(x = RunMonth, y = `OnTime?`, color = `AppUser?`)) + 
  stat_summary(fun = mean, geom = &#39;line&#39;) + 
  scale_x_continuous(breaks = seq(min(Gamification$RunMonth), max(Gamification$RunMonth), by = 1))</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-49-1.png" width="672" />
<strong>Interpretation:</strong> It seems parallel trends assumption
appears to holds. The average proportions of on-time bill payment for
both app users and bank users not using application shows similar
increasing patterns during the control period. This means that the
differences between the two different groups remain consistent over time
before the treatmnet begins. After the control period, However, the
proportion of on-time bill payment sharply decreases among the app users
and it appears that the availability of new games to the app users may
have distracted them to pay bill on-time. In contrast, the patterns
shown on the bank users not using the application have shown similar
patterns, indicating what would have happened if new games had not been
introduced to the application.</p>
<p><br></p>
</div>
<div id="d-model-estimation" class="section level4">
<h4>(d) Model Estimation</h4>
<pre class="r"><code>### Statistical Test 
PTrends &lt;- lm(Gamification$`OnTime?` ~ as.factor(Gamification$RunMonth)*Gamification$TreatGroup)
summary(PTrends)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Gamification$`OnTime?` ~ as.factor(Gamification$RunMonth) * 
##     Gamification$TreatGroup)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.9090  0.0910  0.2395  0.2500  0.2555 
## 
## Coefficients:
##                                                                Estimate
## (Intercept)                                                    0.744500
## as.factor(Gamification$RunMonth)2                              0.010000
## as.factor(Gamification$RunMonth)3                              0.005500
## as.factor(Gamification$RunMonth)4                              0.002000
## as.factor(Gamification$RunMonth)5                              0.016000
## as.factor(Gamification$RunMonth)6                              0.006500
## Gamification$TreatGroupTRUE                                    0.144500
## as.factor(Gamification$RunMonth)2:Gamification$TreatGroupTRUE  0.001000
## as.factor(Gamification$RunMonth)3:Gamification$TreatGroupTRUE  0.012500
## as.factor(Gamification$RunMonth)4:Gamification$TreatGroupTRUE  0.018000
## as.factor(Gamification$RunMonth)5:Gamification$TreatGroupTRUE -0.160000
## as.factor(Gamification$RunMonth)6:Gamification$TreatGroupTRUE -0.147000
##                                                               Std. Error
## (Intercept)                                                     0.008794
## as.factor(Gamification$RunMonth)2                               0.012437
## as.factor(Gamification$RunMonth)3                               0.012437
## as.factor(Gamification$RunMonth)4                               0.012437
## as.factor(Gamification$RunMonth)5                               0.012437
## as.factor(Gamification$RunMonth)6                               0.012437
## Gamification$TreatGroupTRUE                                     0.012437
## as.factor(Gamification$RunMonth)2:Gamification$TreatGroupTRUE   0.017589
## as.factor(Gamification$RunMonth)3:Gamification$TreatGroupTRUE   0.017589
## as.factor(Gamification$RunMonth)4:Gamification$TreatGroupTRUE   0.017589
## as.factor(Gamification$RunMonth)5:Gamification$TreatGroupTRUE   0.017589
## as.factor(Gamification$RunMonth)6:Gamification$TreatGroupTRUE   0.017589
##                                                               t value Pr(&gt;|t|)
## (Intercept)                                                    84.655   &lt;2e-16
## as.factor(Gamification$RunMonth)2                               0.804    0.421
## as.factor(Gamification$RunMonth)3                               0.442    0.658
## as.factor(Gamification$RunMonth)4                               0.161    0.872
## as.factor(Gamification$RunMonth)5                               1.286    0.198
## as.factor(Gamification$RunMonth)6                               0.523    0.601
## Gamification$TreatGroupTRUE                                    11.618   &lt;2e-16
## as.factor(Gamification$RunMonth)2:Gamification$TreatGroupTRUE   0.057    0.955
## as.factor(Gamification$RunMonth)3:Gamification$TreatGroupTRUE   0.711    0.477
## as.factor(Gamification$RunMonth)4:Gamification$TreatGroupTRUE   1.023    0.306
## as.factor(Gamification$RunMonth)5:Gamification$TreatGroupTRUE  -9.097   &lt;2e-16
## as.factor(Gamification$RunMonth)6:Gamification$TreatGroupTRUE  -8.358   &lt;2e-16
##                                                                  
## (Intercept)                                                   ***
## as.factor(Gamification$RunMonth)2                                
## as.factor(Gamification$RunMonth)3                                
## as.factor(Gamification$RunMonth)4                                
## as.factor(Gamification$RunMonth)5                                
## as.factor(Gamification$RunMonth)6                                
## Gamification$TreatGroupTRUE                                   ***
## as.factor(Gamification$RunMonth)2:Gamification$TreatGroupTRUE    
## as.factor(Gamification$RunMonth)3:Gamification$TreatGroupTRUE    
## as.factor(Gamification$RunMonth)4:Gamification$TreatGroupTRUE    
## as.factor(Gamification$RunMonth)5:Gamification$TreatGroupTRUE ***
## as.factor(Gamification$RunMonth)6:Gamification$TreatGroupTRUE ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3933 on 23988 degrees of freedom
## Multiple R-squared:  0.03203,    Adjusted R-squared:  0.03159 
## F-statistic: 72.17 on 11 and 23988 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><span class="math display">\[
\begin{align*}
Y &amp;= 0.7445 + 0.01 \cdot I[\text{RunMonth} = 2] + 0.0055 \cdot
I[\text{RunMonth} = 3] \\
  &amp;+ 0.002 \cdot I[\text{RunMonth} = 4] + 0.016 \cdot
I[\text{RunMonth} = 5] \\
  &amp;+ 0.0065 \cdot I[\text{RunMonth} = 6] + 0.1445 \cdot
I[\text{TreatGroup}] \\
  &amp;+ 0.001 \cdot I[\text{RunMonth} = 2] \cdot I[\text{TreatGroup}]
\\
  &amp;+ 0.0125 \cdot I[\text{RunMonth} = 3] \cdot I[\text{TreatGroup}]
\\
  &amp;+ 0.018 \cdot I[\text{RunMonth} = 4] \cdot I[\text{TreatGroup}]
\\
  &amp;- 0.16 \cdot I[\text{RunMonth} = 5] \cdot I[\text{TreatGroup}] \\
  &amp;- 0.147 \cdot I[\text{RunMonth} = 6] \cdot I[\text{TreatGroup}]
\end{align*}
\]</span> <strong>Interpretation:</strong> If the treatment period is
from Month5 to Month6, the difference between the control group and the
treatment group in a specific month during the control period must not
be statistically significant and this difference is applied in beta
coefficients of the treatment groups except for the baseline
effect(Oct.2021). Here in this model, the beta coefficients for the
treatment group during the control period except the baseline month are
not statistically significant whereas the beta coefficients for the
treatment group during the treatment period are statistically
significant, indicating that the parallel trends assumption holds during
the control period while the true difference between the control group
and the treatment group over months during the treatment period are
validated.</p>
</div>
</div>
<div id="conclusion-8" class="section level3">
<h3>5. Conclusion</h3>
<pre class="r"><code>### DiD (Difference in Differences)
DID1 &lt;- lm(Gamification$`OnTime?` ~ Gamification$TreatPer*Gamification$TreatGroup)
summary(DID1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Gamification$`OnTime?` ~ Gamification$TreatPer * 
##     Gamification$TreatGroup)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.90125  0.09875  0.24425  0.25112  0.25325 
## 
## Coefficients:
##                                                        Estimate Std. Error
## (Intercept)                                            0.748875   0.004397
## Gamification$TreatPerTRUE                              0.006875   0.007616
## Gamification$TreatGroupTRUE                            0.152375   0.006218
## Gamification$TreatPerTRUE:Gamification$TreatGroupTRUE -0.161375   0.010770
##                                                       t value Pr(&gt;|t|)    
## (Intercept)                                           170.318   &lt;2e-16 ***
## Gamification$TreatPerTRUE                               0.903    0.367    
## Gamification$TreatGroupTRUE                            24.505   &lt;2e-16 ***
## Gamification$TreatPerTRUE:Gamification$TreatGroupTRUE -14.983   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3933 on 23996 degrees of freedom
## Multiple R-squared:  0.03185,    Adjusted R-squared:  0.03173 
## F-statistic: 263.1 on 3 and 23996 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><span class="math display">\[
Y = 0.748875
+ 0.006875 \cdot I[\text{TreatPer} = \text{TRUE}]
+ 0.152375 \cdot I[\text{TreatGroup} = \text{TRUE}]
- 0.161375 \cdot I[\text{TreatPer} = \text{TRUE}] \cdot
I[\text{TreatGroup} = \text{TRUE}]
\]</span> <strong>Interpretation:</strong> At a 0.05 significance level,
introducing gamification features to the app statistically significantly
decreases on-time payments by <strong>16.14 percentage points</strong>
among app users, suggesting potential distractions caused by games.The
intercept 0.748875 indicates the proportion of bill payment on-time of
the bank users not using the application during the control period (no
new games availability). 0.006875 indicates the proportion of bill
payment on-time slightly increases, leading to the total proportion of
bill payment on-time of the bank users not using the application during
the treatment period is 0.75575. In contrast, the proportion of bill
payment on-time of the treatment group using the application is 0.152375
larger than the control group during the treatment period and -0.161375
indicates the true effect on the proportion of bill payment on-time
between the control group and the treatment group across periods which
is difference in differences in the proportion of bill payment on-time
between the two due to the new introduction of games. This decreases
user’s ability to pay bills on time.</p>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="difference-in-differences-2" class="section level2">
<h2>14. Difference in Differences</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-11" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/Drug.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Branded Prescriptions: Impact of Disclosure Policy on
Massachusetts Doctors]
</p>
<p><br></p>
</div>
<div id="goal-9" class="section level3">
<h3>2. Goal</h3>
<p>To determine whether a new law requiring disclosure of spending on
doctors discouraged unnecessary branded prescriptions compared to
generic prescriptions.</p>
<p><br></p>
</div>
<div id="hypotheses-4" class="section level3">
<h3>3. Hypotheses</h3>
<p><strong>Null Hypothesis (H₀):</strong></p>
<p>The disclosure policy does not reduce the number of branded
prescriptions: <span class="math display">\[
H_0: \Delta \text{BrandedPrescriptions}_{MA} - \Delta
\text{BrandedPrescriptions}_{NY} \geq 0
\]</span></p>
<p><strong>Alternative Hypothesis (H₁):</strong></p>
<p>The disclosure policy reduces the number of branded prescriptions:
<span class="math display">\[
H_1: \Delta \text{BrandedPrescriptions}_{MA} - \Delta
\text{BrandedPrescriptions}_{NY} &lt; 0
\]</span></p>
<p><br></p>
</div>
<div id="methodology-summary-4" class="section level3">
<h3>4. Methodology &amp; Summary</h3>
<div id="a-data-preparation-3" class="section level4">
<h4>(a) Data Preparation</h4>
<pre class="r"><code>library(dplyr)
library(ggplot2)
### Load data :: This data will include data frames for all three examples 
load(&quot;DiD.Rdata&quot;)
### Create Treatment Period and Treatment Group variable 
Pharma$TreatPer = Pharma$Month &gt;= 6
Pharma$TreatGroup = Pharma$State == &#39;MA&#39;
### Summarize Y by Group 
summary_table &lt;- Pharma %&gt;%
  group_by(TreatGroup, TreatPer) %&gt;%
  summarize(Pres = mean(Prescrip, na.rm =TRUE), .groups = &#39;drop&#39;)
print(summary_table)</code></pre>
<pre><code>## # A tibble: 4 × 3
##   TreatGroup TreatPer  Pres
##   &lt;lgl&gt;      &lt;lgl&gt;    &lt;dbl&gt;
## 1 FALSE      FALSE     39.3
## 2 FALSE      TRUE      38.8
## 3 TRUE       FALSE     36.2
## 4 TRUE       TRUE      32.1</code></pre>
</div>
<div id="b-results-2" class="section level4">
<h4>(b) Results</h4>
<p>Treatment group: Massachusetts Doctors Control group: New York
Doctors</p>
<p>Control period: Jan-May 2009 Treatment period: June-Dec 2009</p>
<p>The number of branded prescriptions by New York Doctors during the
contorl period (no disclosure of spending on doctors) is 39.2760 on
average and this figure slightly decreases to 38.78286 on average during
the treatment period for New York doctors. In contrast, the average
number of branded descriptions by Massachusetts doctors during the
control period is 36.23467 whereas the number of branded descriptions
significantly decreses to 32.08762 when the disclosure requirements was
introduced. This suggests that the disclosure of spending on doctors may
discourage Massachusetts doctors from prescribing branded
prescriptions.</p>
</div>
<div id="c-parallel-trends-assumption-1" class="section level4">
<h4>(c) Parallel Trends Assumption</h4>
<pre class="r"><code>### Parallel trends assumption
ggplot(Pharma, aes(x = Month, y = Prescrip, color = State)) +
  stat_summary(fun = mean, geom = &#39;line&#39;) +
  scale_x_continuous(breaks = seq(min(Pharma$Month), max(Pharma$Month), by = 1))</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-53-1.png" width="672" /></p>
<p><strong>Interpretation:</strong> It appears that the parallel trends
assumption holds during the control period from Jan - May 2009 as both
groups exhibit similar decreasing patterns. After the control period,
the number of branded prescriptions by Massachusetts doctors sharply
decreases compared to that of New York doctors. This suggest that
disclosure policy may have had a significant impact on reducing branded
prescriptions in the treatment group.</p>
</div>
<div id="d-model-estimation-1" class="section level4">
<h4>(d) Model Estimation</h4>
<pre class="r"><code>### Statistical Test
PTrends &lt;- lm(Pharma$Prescrip ~ as.factor(Pharma$Month) * Pharma$TreatGroup)
summary(PTrends)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Pharma$Prescrip ~ as.factor(Pharma$Month) * Pharma$TreatGroup)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -18.473  -3.713  -0.060   3.653  28.527 
## 
## Coefficients:
##                                                 Estimate Std. Error t value
## (Intercept)                                     40.44667    0.45201  89.482
## as.factor(Pharma$Month)2                        -0.28667    0.63924  -0.448
## as.factor(Pharma$Month)3                        -1.10667    0.63924  -1.731
## as.factor(Pharma$Month)4                        -2.01333    0.63924  -3.150
## as.factor(Pharma$Month)5                        -2.44667    0.63924  -3.827
## as.factor(Pharma$Month)6                        -3.97333    0.63924  -6.216
## as.factor(Pharma$Month)7                        -3.65333    0.63924  -5.715
## as.factor(Pharma$Month)8                        -2.74000    0.63924  -4.286
## as.factor(Pharma$Month)9                        -0.73333    0.63924  -1.147
## as.factor(Pharma$Month)10                       -0.36000    0.63924  -0.563
## as.factor(Pharma$Month)11                       -0.12000    0.63924  -0.188
## as.factor(Pharma$Month)12                       -0.06667    0.63924  -0.104
## Pharma$TreatGroupTRUE                           -3.00000    0.63924  -4.693
## as.factor(Pharma$Month)2:Pharma$TreatGroupTRUE  -0.30000    0.90402  -0.332
## as.factor(Pharma$Month)3:Pharma$TreatGroupTRUE   0.10667    0.90402   0.118
## as.factor(Pharma$Month)4:Pharma$TreatGroupTRUE  -0.07333    0.90402  -0.081
## as.factor(Pharma$Month)5:Pharma$TreatGroupTRUE   0.06000    0.90402   0.066
## as.factor(Pharma$Month)6:Pharma$TreatGroupTRUE  -4.31333    0.90402  -4.771
## as.factor(Pharma$Month)7:Pharma$TreatGroupTRUE  -3.44667    0.90402  -3.813
## as.factor(Pharma$Month)8:Pharma$TreatGroupTRUE  -3.11333    0.90402  -3.444
## as.factor(Pharma$Month)9:Pharma$TreatGroupTRUE  -4.02000    0.90402  -4.447
## as.factor(Pharma$Month)10:Pharma$TreatGroupTRUE -3.79333    0.90402  -4.196
## as.factor(Pharma$Month)11:Pharma$TreatGroupTRUE -3.49333    0.90402  -3.864
## as.factor(Pharma$Month)12:Pharma$TreatGroupTRUE -3.68667    0.90402  -4.078
##                                                 Pr(&gt;|t|)    
## (Intercept)                                      &lt; 2e-16 ***
## as.factor(Pharma$Month)2                        0.653855    
## as.factor(Pharma$Month)3                        0.083497 .  
## as.factor(Pharma$Month)4                        0.001648 ** 
## as.factor(Pharma$Month)5                        0.000132 ***
## as.factor(Pharma$Month)6                        5.70e-10 ***
## as.factor(Pharma$Month)7                        1.19e-08 ***
## as.factor(Pharma$Month)8                        1.86e-05 ***
## as.factor(Pharma$Month)9                        0.251375    
## as.factor(Pharma$Month)10                       0.573354    
## as.factor(Pharma$Month)11                       0.851104    
## as.factor(Pharma$Month)12                       0.916944    
## Pharma$TreatGroupTRUE                           2.79e-06 ***
## as.factor(Pharma$Month)2:Pharma$TreatGroupTRUE  0.740021    
## as.factor(Pharma$Month)3:Pharma$TreatGroupTRUE  0.906081    
## as.factor(Pharma$Month)4:Pharma$TreatGroupTRUE  0.935352    
## as.factor(Pharma$Month)5:Pharma$TreatGroupTRUE  0.947087    
## as.factor(Pharma$Month)6:Pharma$TreatGroupTRUE  1.90e-06 ***
## as.factor(Pharma$Month)7:Pharma$TreatGroupTRUE  0.000140 ***
## as.factor(Pharma$Month)8:Pharma$TreatGroupTRUE  0.000580 ***
## as.factor(Pharma$Month)9:Pharma$TreatGroupTRUE  8.98e-06 ***
## as.factor(Pharma$Month)10:Pharma$TreatGroupTRUE 2.78e-05 ***
## as.factor(Pharma$Month)11:Pharma$TreatGroupTRUE 0.000113 ***
## as.factor(Pharma$Month)12:Pharma$TreatGroupTRUE 4.64e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.536 on 3576 degrees of freedom
## Multiple R-squared:  0.2604, Adjusted R-squared:  0.2556 
## F-statistic: 54.73 on 23 and 3576 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><span class="math display">\[
\begin{aligned}
Y &amp;= 40.44667 \\
  &amp;\quad - 0.28667 \cdot I[\text{Month} = 2] \\
  &amp;\quad - 1.10667 \cdot I[\text{Month} = 3] \\
  &amp;\quad - 2.01333 \cdot I[\text{Month} = 4] \\
  &amp;\quad - 2.44667 \cdot I[\text{Month} = 5] \\
  &amp;\quad - 3.97333 \cdot I[\text{Month} = 6] \\
  &amp;\quad - 3.65333 \cdot I[\text{Month} = 7] \\
  &amp;\quad - 2.74000 \cdot I[\text{Month} = 8] \\
  &amp;\quad - 0.73333 \cdot I[\text{Month} = 9] \\
  &amp;\quad - 0.36000 \cdot I[\text{Month} = 10] \\
  &amp;\quad - 0.12000 \cdot I[\text{Month} = 11] \\
  &amp;\quad - 0.06667 \cdot I[\text{Month} = 12] \\
  &amp;\quad - 3.00000 \cdot I[\text{TreatGroup}] \\
  &amp;\quad - 0.30000 \cdot I[\text{Month} = 2 \text{ and TreatGroup}]
\\
  &amp;\quad + 0.10667 \cdot I[\text{Month} = 3 \text{ and TreatGroup}]
\\
  &amp;\quad - 0.07333 \cdot I[\text{Month} = 4 \text{ and TreatGroup}]
\\
  &amp;\quad + 0.06000 \cdot I[\text{Month} = 5 \text{ and TreatGroup}]
\\
  &amp;\quad - 4.31333 \cdot I[\text{Month} = 6 \text{ and TreatGroup}]
\\
  &amp;\quad - 3.44667 \cdot I[\text{Month} = 7 \text{ and TreatGroup}]
\\
  &amp;\quad - 3.11333 \cdot I[\text{Month} = 8 \text{ and TreatGroup}]
\\
  &amp;\quad - 4.02000 \cdot I[\text{Month} = 9 \text{ and TreatGroup}]
\\
  &amp;\quad - 3.79333 \cdot I[\text{Month} = 10 \text{ and TreatGroup}]
\\
  &amp;\quad - 3.49333 \cdot I[\text{Month} = 11 \text{ and TreatGroup}]
\\
  &amp;\quad - 3.68667 \cdot I[\text{Month} = 12 \text{ and TreatGroup}]
\end{aligned}
\]</span> <strong>Interpretation:</strong> If the parallel trends
assupmtion holds in this regression model, the beta coefficients for the
treatment group during the control period except for the baseline month
which is Jan 2009 are not statistically significant, indicating the
differences between the control group and the treatment group during the
control period should remain consistent. However, the differences
between the two groups during the treatment period should be
statistically significant if there is any true effect of new disclosure
policy during the treatment period to reduce the number of branded
prescriptions by the doctors in the treatment group compared to the
control group.</p>
<p><br></p>
</div>
</div>
<div id="conclusion-9" class="section level3">
<h3>5. Conclusion</h3>
<pre class="r"><code>DID1 &lt;- lm(Pharma$Prescrip ~ Pharma$TreatPer*Pharma$TreatGroup)
summary(DID1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Pharma$Prescrip ~ Pharma$TreatPer * Pharma$TreatGroup)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -20.7829  -4.0876  -0.2347   3.7653  26.2171 
## 
## Coefficients:
##                                           Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)                                39.2760     0.2078 189.007   &lt;2e-16
## Pharma$TreatPerTRUE                        -0.4931     0.2721  -1.813     0.07
## Pharma$TreatGroupTRUE                      -3.0413     0.2939 -10.349   &lt;2e-16
## Pharma$TreatPerTRUE:Pharma$TreatGroupTRUE  -3.6539     0.3848  -9.496   &lt;2e-16
##                                              
## (Intercept)                               ***
## Pharma$TreatPerTRUE                       .  
## Pharma$TreatGroupTRUE                     ***
## Pharma$TreatPerTRUE:Pharma$TreatGroupTRUE ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 5.691 on 3596 degrees of freedom
## Multiple R-squared:  0.214,  Adjusted R-squared:  0.2134 
## F-statistic: 326.4 on 3 and 3596 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><span class="math display">\[
Y = 39.2760 - 0.4931 \cdot I[\text{TreatPer}] - 3.0413 \cdot
I[\text{TreatGroup}] - 3.6539 \cdot I[\text{TreatPer}] \times
I[\text{TreatGroup}]
\]</span></p>
<p><strong>Interpretation:</strong> At a 0.05 significance level, the
disclosure policy significantly reduces the number of branded
prescriptions for Massachusetts doctors by <strong>3.65 units</strong>
compared to New York doctors. The intercept 39.2760 indicates the number
of branded prescriptions by doctors in the control group during the
control period without requiring any disclosure of spending on doctors.
This figure slightly decreases by 0.4931 during the treatment period for
the doctors in the control group. This is the difference in the number
of branded prescriptions by doctors in the control group between the
control period and the treatment period. During the control period, the
number of branded prescriptions by doctors in the treatment group
decreases by 3.0413 compared to that of the doctors in the control
group. After the treatment period introducing new disclosure policy, the
difference in differences between the control group and the treatment
group across periods is -3.6539, indicating that the true effect of new
policy introduction in the treatment group during the treatment period
compared to the difference in the control group occurred across periods
reduces the number of branded prescriptions by doctors in the treatment
group.</p>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="staggered-difference-in-differences" class="section level2">
<h2>14. Staggered Difference in Differences</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="model-specification" class="section level3">
<h3>1. Model Specification</h3>
<p><span class="math display">\[
Y_{ct} = \alpha + \alpha_{t=2} \cdot I[t = 2] + \alpha_{t=3} \cdot I[t =
3] + \alpha_{c=2} \cdot I[c = 2] + \alpha_{c=3} \cdot I[c = 3] + \beta
\cdot Treat_{ct}
\]</span></p>
</div>
<div id="components" class="section level3">
<h3>2. Components</h3>
<ol style="list-style-type: decimal">
<li><span class="math display">\[
Y_{ct}
\]</span></li>
</ol>
<ul>
<li>This is dependent value at a specific time t for a city c,
representing the outcome for city c at time t.</li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li><span class="math display">\[
\alpha
\]</span></li>
</ol>
<ul>
<li>This is the baseline effect or reference outcome for the baseline
city (c=1) and the baseline time period (time 1) when there is no
time-specific and city-specific variation.</li>
</ul>
<ol start="3" style="list-style-type: decimal">
<li><span class="math display">\[
\alpha_{t=2} \cdot I[t = 2]
\]</span></li>
</ol>
<ul>
<li>This indicates the time fixed effects in a time period 2 compared to
the baseline period (t=1) independent of city-specific effects or
treatment effect. It indicates how outcomes change across time periods
independent of city-specific effects or treatment effect.</li>
</ul>
<ol start="4" style="list-style-type: decimal">
<li><span class="math display">\[
\alpha_{t=3} \cdot I[t = 3]
\]</span></li>
</ol>
<ul>
<li>This indicates the time fixed effect relative to the baseline period
(t=1). It indicates how outcome changes across time periods independent
of treatment effects or city-specific effects.</li>
</ul>
<ol start="5" style="list-style-type: decimal">
<li><span class="math display">\[
\alpha_{c=2} \cdot I[c = 2]
\]</span></li>
</ol>
<ul>
<li>This indicates city fixed effect compared to the baseline city
effect (c=1). This indicates how outcome changes between the two
different cities independent of time-specific effects or treatment
effects.</li>
</ul>
<ol start="6" style="list-style-type: decimal">
<li><span class="math display">\[
\alpha_{c=3} \cdot I[c = 3]
\]</span></li>
</ol>
<ul>
<li>This indicates city fixed effect compared to the baseline city
effect (c=1). This indicates how outcome changes between the two
different cities independent of time-specific effects or treatment
effects.</li>
</ul>
<ol start="7" style="list-style-type: decimal">
<li><span class="math display">\[
\beta \cdot Treat_{ct}
\]</span></li>
</ol>
<ul>
<li>This is the treatment effect introduced in a specific time period
and a specific city. The beta coefficient indicates the difference in th
outcome caused by the treatment is introduced in a specific time period
and a city compared to the baseline city (city 1) and baseline time
period (time 1).</li>
</ul>
</div>
<div id="dynamic-treatment-and-visualization" class="section level3">
<h3>3. Dynamic Treatment and Visualization</h3>
<div id="a-pre-treatment-trends-test" class="section level4">
<h4>(a) Pre-Treatment Trends Test</h4>
<ul>
<li>Dynamic Treatment</li>
</ul>
<p>Two-Way Fixed effect Models cannot accurately capture treatment
effect estimates when treatment effects are dynamic (the effect size
changes over time) and when treatment is staggered.</p>
<pre class="r"><code>### Parallel trends test using just first 6 months 
library(ggplot2)
load(&#39;StaggeredDiD.Rdata&#39;)
ptrend &lt;- lm(Spend ~ factor(GameID)*factor(MonthID), data = TWFE_Data[TWFE_Data$MonthID&lt;=6,])
summary(ptrend)</code></pre>
<pre><code>## 
## Call:
## lm(formula = Spend ~ factor(GameID) * factor(MonthID), data = TWFE_Data[TWFE_Data$MonthID &lt;= 
##     6, ])
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -2.187 -1.614 -1.474  2.840 27.906 
## 
## Coefficients:
##                                  Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                       1.57900    0.04265  37.027  &lt; 2e-16 ***
## factor(GameID)2                   0.14400    0.06031   2.388  0.01696 *  
## factor(GameID)3                  -0.17000    0.06031  -2.819  0.00482 ** 
## factor(GameID)4                  -0.07600    0.06031  -1.260  0.20761    
## factor(GameID)5                   0.48600    0.06031   8.058 7.78e-16 ***
## factor(MonthID)2                  0.00600    0.06031   0.099  0.92075    
## factor(MonthID)3                 -0.41200    0.06031  -6.831 8.44e-12 ***
## factor(MonthID)4                 -0.00100    0.06031  -0.017  0.98677    
## factor(MonthID)5                 -0.01500    0.06031  -0.249  0.80358    
## factor(MonthID)6                  0.03500    0.06031   0.580  0.56168    
## factor(GameID)2:factor(MonthID)2  0.05100    0.08529   0.598  0.54987    
## factor(GameID)3:factor(MonthID)2  0.04300    0.08529   0.504  0.61415    
## factor(GameID)4:factor(MonthID)2 -0.05200    0.08529  -0.610  0.54207    
## factor(GameID)5:factor(MonthID)2  0.02700    0.08529   0.317  0.75157    
## factor(GameID)2:factor(MonthID)3 -0.01700    0.08529  -0.199  0.84201    
## factor(GameID)3:factor(MonthID)3  0.14200    0.08529   1.665  0.09593 .  
## factor(GameID)4:factor(MonthID)3  0.11200    0.08529   1.313  0.18913    
## factor(GameID)5:factor(MonthID)3 -0.06700    0.08529  -0.786  0.43213    
## factor(GameID)2:factor(MonthID)4  0.04700    0.08529   0.551  0.58159    
## factor(GameID)3:factor(MonthID)4  0.06500    0.08529   0.762  0.44600    
## factor(GameID)4:factor(MonthID)4  0.06100    0.08529   0.715  0.47448    
## factor(GameID)5:factor(MonthID)4  0.09600    0.08529   1.126  0.26035    
## factor(GameID)2:factor(MonthID)5  0.01400    0.08529   0.164  0.86962    
## factor(GameID)3:factor(MonthID)5  0.08000    0.08529   0.938  0.34826    
## factor(GameID)4:factor(MonthID)5  0.05400    0.08529   0.633  0.52665    
## factor(GameID)5:factor(MonthID)5  0.13700    0.08529   1.606  0.10821    
## factor(GameID)2:factor(MonthID)6 -0.05700    0.08529  -0.668  0.50394    
## factor(GameID)3:factor(MonthID)6 -0.01900    0.08529  -0.223  0.82372    
## factor(GameID)4:factor(MonthID)6  0.00800    0.08529   0.094  0.92527    
## factor(GameID)5:factor(MonthID)6 -0.00600    0.08529  -0.070  0.94392    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.015 on 149970 degrees of freedom
## Multiple R-squared:  0.00826,    Adjusted R-squared:  0.008068 
## F-statistic: 43.07 on 29 and 149970 DF,  p-value: &lt; 2.2e-16</code></pre>
<ul>
<li>Plot pre-treatment trends (first 6 months)</li>
</ul>
<pre class="r"><code>library(ggplot2)
load(&#39;StaggeredDiD.Rdata&#39;)
ggplot(DigiGame[DigiGame$MonthID&lt;=6,], aes(x = MonthID)) +
  stat_summary(aes(y = Game1), fun = mean, geom = &quot;line&quot;, color = &quot;blue&quot;) + 
  stat_summary(aes(y = Game2), fun = mean, geom = &quot;line&quot;, color = &quot;red&quot;) + stat_summary(aes(y = Game3), fun = mean, geom = &#39;line&#39;, color = &quot;green&quot;) + stat_summary(aes(y = Game4), fun = mean, geom = &#39;line&#39;, color = &#39;purple&#39;) + stat_summary(aes(y = Game5), fun = mean, geom = &#39;line&#39;, color = &quot;orange&quot;)</code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-57-1.png" width="672" />
- Prior to the treatment effects, Game1 ~ 5 shows similar patterns in
average spending during the six months period proving that the parallel
trends assumption holds. This indicates the differences among games
before the treatment effects was consistent over months.</p>
</div>
<div id="b-full-time-series-mean-centered" class="section level4">
<h4>(b) Full Time Series, Mean-Centered</h4>
<pre class="r"><code>### Plot entire time series, mean-centered to see deviation
DigiGame_MC &lt;-  DigiGame
DigiGame_MC$Game1 &lt;- DigiGame$Game1 - mean(DigiGame$Game1[DigiGame$MonthID&lt;=6])
DigiGame_MC$Game2 &lt;- DigiGame$Game2 - mean(DigiGame$Game2[DigiGame$MonthID&lt;=6])
DigiGame_MC$Game3 &lt;- DigiGame$Game3 - mean(DigiGame$Game3[DigiGame$MonthID&lt;=6])
DigiGame_MC$Game4 &lt;- DigiGame$Game4 - mean(DigiGame$Game4[DigiGame$MonthID&lt;=6])
DigiGame_MC$Game5 &lt;- DigiGame$Game5 - mean(DigiGame$Game5[DigiGame$MonthID&lt;=6])


ggplot(DigiGame_MC, aes(x = MonthID)) +
  stat_summary(aes(y = Game1), fun = mean, geom = &#39;line&#39;, color = &#39;blue&#39;) +
  stat_summary(aes(y = Game2), fun = mean, geom = &#39;line&#39;, color = &#39;red&#39;) +
  stat_summary(aes(y = Game3), fun = mean, geom = &#39;line&#39;, color = &#39;green&#39;) +
  stat_summary(aes(y = Game4), fun = mean, geom = &#39;line&#39;, color = &#39;purple&#39;) +
  stat_summary(aes(y = Game5), fun = mean, geom = &#39;line&#39;, color = &#39;orange&#39;) </code></pre>
<p><img src="stat2_files/figure-html/unnamed-chunk-58-1.png" width="672" />
- Mean centering is necessary to remove the differences in the dependent
values among different games during the control period to make them
started from 0 in order to identify the true treatment staggered effects
in each game during the treatment periods. - Depending on the staggered
treatment effects during the treatment periods, Game 1 received
treatment effect starting around 7 months. Game 2 received treatment
effect starting around 13 months. Game 3 received treatment effect
starting around 16 months. Game 4 received treatment effect starting
around 19 months. Game 5 received treatment effect starting around 22
months.</p>
</div>
</div>
<div id="conclusion-10" class="section level3">
<h3>4. Conclusion</h3>
<p>Dynamic staggered treatment effects are visualized and tested,
ensuring parallel trends hold and identifying the timing of treatment
effects for each game.</p>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="customer-dynamics-and-pricing" class="section level2">
<h2>15. Customer Dynamics and Pricing</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-12" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/discounts.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Branded Prescriptions: Impact of Disclosure Policy on
Massachusetts Doctors]
</p>
<p><br></p>
</div>
<div id="goal-10" class="section level3">
<h3>2. Goal</h3>
<p>To measure how temporary discounts on one-week and two-week lagged
prices impact weekly sales for Reese’s and Kit Kat brands.</p>
<p><br></p>
</div>
<div id="methodology-5" class="section level3">
<h3>3. Methodology</h3>
<ol style="list-style-type: decimal">
<li><strong>Lagging Prices:</strong>
<ul>
<li>Created <strong>one-week prior price</strong>
(<code>Kpricet1</code>, <code>Rpricet1</code>) and <strong>two-week
prior price</strong> (<code>Kpricet2</code>,
<code>Rpricet2</code>).</li>
<li>Set baseline price to $1.49 for weeks without a discount.</li>
</ul></li>
<li><strong>Encoding Price Levels:</strong>
<ul>
<li>0: Stable price (baseline of $1.49).</li>
<li>-1: Discounted price ($1.29).</li>
</ul></li>
<li><strong>Regression Models:</strong>
<ul>
<li>Modeled <strong>Kit Kat Sales</strong> and <strong>Reese’s
Sales</strong> as functions of current and lagged prices.</li>
</ul></li>
</ol>
<p><br></p>
<p><br><br><br></p>
</div>
<div id="code-8" class="section level3">
<h3>4. Code</h3>
<pre class="r"><code>getwd()</code></pre>
<pre><code>## [1] &quot;C:/Github/jennyjihyunseo.github.io&quot;</code></pre>
<pre class="r"><code>load(&quot;Lecture8.Rdata&quot;)</code></pre>
<pre class="r"><code>### Create Lags
### Create Price vector containing the last week price included in the last element of the vector
### Include $1.49 as the first week of price
Stockpiling$Kpricet1 &lt;- c(1.49, Stockpiling$`Kit Kat Price`[-length(Stockpiling$`Kit Kat Price`)])
### Create two weeks before price in the last element of the price vector
### Include $1.49 as the first week of price 
Stockpiling$Kpricet2 &lt;- c(1.49, Stockpiling$Kpricet1[-length(Stockpiling$Kpricet1)])
### Create one week before price in the last element of the price vector
### Include $1.49 as the placeholder of the first week price
Stockpiling$Rpricet1 &lt;- c(1.49, Stockpiling$`Reese&#39;s Price`[-length(Stockpiling$`Reese&#39;s Price`)])
### Create two weeks before price in the last element of the price vector
### Include $1.49 as the placeholder of the first week price
Stockpiling$Rpricet2 &lt;- c(1.49, Stockpiling$Rpricet1[-length(Stockpiling$`Rpricet1`)])

### Encode Price levels without using actual numerical values 
### 0 indicates the price remained stable (baseline of 1.49)
### 1 indicates a discount was applied (price reduced to 1.29)

Stockpiling$Kpricet1[Stockpiling$Kpricet1 == 1.49] &lt;- 0
Stockpiling$Kpricet1[Stockpiling$Kpricet1 == 1.29] &lt;- -1
Stockpiling$Kpricet2[Stockpiling$Kpricet2 == 1.49] &lt;- 0
Stockpiling$Kpricet2[Stockpiling$Kpricet2 == 1.29] &lt;- -1
Stockpiling$Rpricet1[Stockpiling$Rpricet1 == 1.49] &lt;- 0
Stockpiling$Rpricet1[Stockpiling$Rpricet1 == 1.29] &lt;- -1
Stockpiling$Rpricet2[Stockpiling$Rpricet2 == 1.49] &lt;- 0
Stockpiling$Rpricet2[Stockpiling$Rpricet2 == 1.29] &lt;- -1

OLS_KitKat &lt;- lm(`Kit Kat Sales` ~ `Reese&#39;s Price` + `Kit Kat Price` + Kpricet1 + Kpricet2 + Rpricet1 + Rpricet2, data=Stockpiling)
summary(OLS_KitKat)</code></pre>
<pre><code>## 
## Call:
## lm(formula = `Kit Kat Sales` ~ `Reese&#39;s Price` + `Kit Kat Price` + 
##     Kpricet1 + Kpricet2 + Rpricet1 + Rpricet2, data = Stockpiling)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.7448  -6.4192   0.0227   5.9862  17.6043 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      466.860     17.508  26.665  &lt; 2e-16 ***
## `Reese&#39;s Price`  148.748      9.308  15.981  &lt; 2e-16 ***
## `Kit Kat Price` -412.818      8.634 -47.813  &lt; 2e-16 ***
## Kpricet1          11.854      1.736   6.827 7.56e-10 ***
## Kpricet2           7.227      1.744   4.144 7.30e-05 ***
## Rpricet1          10.906      1.869   5.836 7.06e-08 ***
## Rpricet2          -2.389      1.885  -1.267    0.208    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.589 on 97 degrees of freedom
## Multiple R-squared:  0.9642, Adjusted R-squared:  0.9619 
## F-statistic: 434.9 on 6 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation:</strong> When all prices are 0, the baseline
Kit Kat sales is 466.860 units. When ‘Reese’s price’ increases by 1
unit, the average Kit Kat sales increases by 148.748 units. In contrast,
when ‘Kit Kat’s price’ increases by 1 unit, the average Kit Kat sales
decreases. When the one week earlier price decreases by 1 unit, the
average Kit Kat sales decreases by 11.854 units. When the two week
earlier price decreases by 1 unit, the average Kit Kat sales decreases
by 7.227 units. When the one week earlier Reese’s price decreases by 1
units, the average Reese sales decreases by 10.906 units. However, when
the two week earlier price decreases by 1 unit, the Kit Kat sales
increases by 2.389 but this coefficient effect is not statistically
significant.</p>
<pre class="r"><code>OLS_Reese &lt;- lm(`Reese&#39;s Sales` ~ `Reese&#39;s Price` + `Kit Kat Price` + Kpricet1 + Kpricet2 + Rpricet1 + Rpricet2, data = Stockpiling )
summary(OLS_Reese)</code></pre>
<pre><code>## 
## Call:
## lm(formula = `Reese&#39;s Sales` ~ `Reese&#39;s Price` + `Kit Kat Price` + 
##     Kpricet1 + Kpricet2 + Rpricet1 + Rpricet2, data = Stockpiling)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.754  -6.372  -1.397   5.274  16.809 
## 
## Coefficients:
##                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)      518.958     17.040  30.455  &lt; 2e-16 ***
## `Reese&#39;s Price` -401.190      9.059 -44.287  &lt; 2e-16 ***
## `Kit Kat Price`  137.454      8.403  16.357  &lt; 2e-16 ***
## Kpricet1          11.563      1.690   6.842 7.04e-10 ***
## Kpricet2           1.931      1.697   1.138    0.258    
## Rpricet1          14.238      1.819   7.829 6.19e-12 ***
## Rpricet2           8.101      1.835   4.415 2.63e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.359 on 97 degrees of freedom
## Multiple R-squared:  0.9634, Adjusted R-squared:  0.9612 
## F-statistic: 425.8 on 6 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p><strong>Interpretation:</strong> When all prices are 0, the baseline
Reese’s sales is 518.958 units. When Reese’s price increases by 1 unit,
the Reese’s sales decreases by 401.190 units on average. When Kit Kat
price increases by 1 unit, the Reese’s sales increases 137.454 units on
average. When one week earlier Kit Kat price decreases by 1 unit,
Reese’s sales decreases by 11.563 on average. When two week earlier Kit
Kat price decreases by 1 unit, the Reese’s sales decreases by 1.931
units on average whereas this coefficient effect is not statistically
significant. When one week ealier Reese’s price decreases by 1 unit, the
Reese’s sales decreases on average by 14.238 units. When two week
earlier Reese’s price decreases by 1 unit, the Reese’s sales increases
by 8.101 units on average.</p>
</div>
<div id="conclude" class="section level3">
<h3>5. Conclude</h3>
<ul>
<li>Cross-Brand Dynamics: Reese’s and Kit Kat sales are highly sensitive
to each other’s prices, suggesting competitive or substitution
effects.</li>
<li>Lagged Price Influence: Sales are significantly affected by
discounts introduced one or two weeks prior, with varying effects
depending on the brand and timing.</li>
<li>Price Elasticity: Reese’s displays greater price sensitivity than
Kit Kat, both for current and lagged prices.</li>
</ul>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="dynamic-pricing" class="section level2">
<h2>16. Dynamic Pricing</h2>
<p><img src="https://img.shields.io/badge/Using-R-blue" /></p>
<p><br></p>
<p><br><br><br></p>
<div id="figure-13" class="section level3">
<h3>1. Figure</h3>
<p align="center">
<img src="images/tickets.png" style="width:80%; height:auto;" align="center">
</p>
<p align="center">
[Fig. Branded Prescriptions: Impact of Disclosure Policy on
Massachusetts Doctors]
</p>
<p><br></p>
</div>
<div id="goal-11" class="section level3">
<h3>2. Goal</h3>
<p>To understand how ticket pricing and the time remaining before the
flight take-off affect the number of seats sold, accounting for: -
Seasonal effects (Spring, Summer, Fall; baseline: Winter). - Interaction
between ticket prices and weeks remaining to flight take-off to examine
changing price sensitivity.</p>
<p><br></p>
</div>
<div id="methodology-6" class="section level3">
<h3>3. Methodology</h3>
<ol style="list-style-type: decimal">
<li><strong>Dummy Variables for Seasons:</strong>
<ul>
<li><strong>Baseline:</strong> Winter.</li>
<li><strong>Spring, Summer, Fall</strong>: Coded as 1 for respective
months, 0 otherwise.</li>
</ul></li>
<li><strong>Weeks Before Flight:</strong>
<ul>
<li>Created dummy variables for 5, 4, 3, and 2 weeks prior to flight
take-off.</li>
<li><strong>Baseline:</strong> 1 week before take-off.</li>
</ul></li>
<li><strong>Interaction Term:</strong>
<ul>
<li>Interaction between ticket prices and weeks before flight to capture
varying price sensitivities over time.</li>
</ul></li>
<li><strong>Regression Model:</strong>
<ul>
<li>Outcome Variable: <code>Seats Sold</code>.</li>
<li>Predictor Variables: <code>Price</code>, <code>Weeks Before</code>,
Seasonal dummies, and Interaction terms.</li>
</ul></li>
</ol>
<p><br></p>
</div>
<div id="code-9" class="section level3">
<h3>4. Code</h3>
<pre class="r"><code>load(&quot;Lecture8.Rdata&quot;)

### Create dummy variables for seasonality except for the baseline season which is winter
Pricing$Spring &lt;- 0
Pricing$Summer &lt;- 0
Pricing$Fall &lt;- 0
# If Spring is 1 , other seasonal dummy variables are 0
Pricing$Spring[Pricing$Month_of_Flight %in% c(&quot;Mar&quot;, &quot;Apr&quot;, &quot;May&quot;)] &lt;- 1
# If Summer is 1, 0 otherwise
Pricing$Summer[Pricing$Month_of_Flight %in% c(&quot;Jun&quot;, &quot;Jul&quot;, &quot;Aug&quot;)] &lt;- 1
# If Fall is 1, 0 otherwise
Pricing$Fall[Pricing$Month_of_Flight %in% c(&quot;Sep&quot;, &quot;Oct&quot;, &quot;Nov&quot;)] &lt;- 1

# Make weeks before as a categorical variable with 4 dummy variables except for the baseline week (1 week before the actual flight take-off)
# Interaction term indicates how the price sensitivity of seat sold varies when the flight take-off approaches 
OLS_Pricing &lt;- lm(`Seats Sold` ~ factor(`Weeks Before`)*Price + Spring + Summer + Fall, data = Pricing)
summary(OLS_Pricing)</code></pre>
<pre><code>## 
## Call:
## lm(formula = `Seats Sold` ~ factor(`Weeks Before`) * Price + 
##     Spring + Summer + Fall, data = Pricing)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -15.2326  -6.5642  -0.4281   7.0383  14.2862 
## 
## Coefficients:
##                                 Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                   89.8980860 12.6359463   7.114 1.46e-11 ***
## factor(`Weeks Before`)2        2.0349021 16.5032742   0.123 0.901976    
## factor(`Weeks Before`)3       25.0910231 17.3652709   1.445 0.149866    
## factor(`Weeks Before`)4       70.9946772 15.9556978   4.449 1.35e-05 ***
## factor(`Weeks Before`)5       68.9872231 17.9072268   3.852 0.000152 ***
## Price                         -0.1336153  0.0278587  -4.796 2.93e-06 ***
## Spring                         7.9672982  1.4958419   5.326 2.41e-07 ***
## Summer                        15.3436880  1.8363439   8.356 6.51e-15 ***
## Fall                          -0.7997980  1.4088108  -0.568 0.570792    
## factor(`Weeks Before`)2:Price  0.0005627  0.0358771   0.016 0.987500    
## factor(`Weeks Before`)3:Price -0.0619034  0.0383103  -1.616 0.107517    
## factor(`Weeks Before`)4:Price -0.1496490  0.0349311  -4.284 2.71e-05 ***
## factor(`Weeks Before`)5:Price -0.1504809  0.0394932  -3.810 0.000179 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.615 on 227 degrees of freedom
## Multiple R-squared:  0.5065, Adjusted R-squared:  0.4804 
## F-statistic: 19.41 on 12 and 227 DF,  p-value: &lt; 2.2e-16</code></pre>
<ul>
<li><p>Price: When price increases by $1, the number of seats sold
decreases by 0.1336153 units on average. Price and the number of seats
sold are inversely proportional.</p></li>
<li><p>Seasonality: the number of seats sold increases in both spring
and summer which are statistically significant compared to the baseline
season as winter. There are more demands during these periods.</p></li>
<li><p>Weeks before the flight take-off: The number of seats sold in 4
and 5 weeks before the actual flight take-off increases by 70.99 and
68.98 units respectively compared to the number of seats sold in the
first week before the flight take-off which are statistically
significant. This result indicates more tickets are sold in the early
booking windows.</p></li>
<li><p>Interaction terms(time * price): When the ticket price increases
by 1 unit in 4 and 5 weeks prior to the flight take-off, the number of
seat sold decreases more than that of 1 week prior to the flight
take-off, which indicates the prices sensitivity increases when more
weeks remaining prior to the take-off whereas the price sensitivity
decreases when the take-off schedule comes much closer because customers
have to secure a seat as soon as possible regardless of price,
reflecting the urgency of the last-minute travel decisions.</p></li>
</ul>
<p><br></p>
</div>
<div id="conclusion-11" class="section level3">
<h3>5. Conclusion</h3>
<ul>
<li>Early Booking: Customers are more price-sensitive when booking
earlier (4-5 weeks before take-off).</li>
<li>Late Booking: Customers prioritize securing a seat over price as the
flight date approaches.</li>
<li>Seasonality: Spring and Summer drive higher demand, highlighting the
need for seasonally adjusted pricing strategies.</li>
<li>Strategic Implications: Airlines should leverage early booking
incentives and adjust prices closer to the flight date to optimize
revenue while accommodating urgency-driven demand.</li>
</ul>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="theoretical-statistics-learning" class="section level2">
<h2>17. Theoretical Statistics Learning</h2>
<p>This document contains structured explanations and analyses of
statistical concepts, experiments, and methods to support theoretical
understanding and practical applications.</p>
<p><br></p>
<div id="experiments" class="section level3">
<h3>1. Experiments</h3>
<p><br></p>
<div
id="why-do-we-randomly-assign-people-to-a-treatment-and-control-group-in-an-experiment"
class="section level4">
<h4>(1) Why do we randomly assign people to a treatment and control
group in an experiment?</h4>
<ul>
<li><strong>Explanation</strong>: If we want to compare two groups to
determine any treatment effect on the dependent variable Y, we have to
control other factors (confounders) except for the treatment effect to
make the two groups identical. For example, if we want to experiment
whether a specific treatment effect is effective by identifying any
difference on the dependent variable, we have to control other factors
and impose the treatment on a treatment group and compare the dependent
variable of Y for each group to determine whether this treatment effect
makes any difference on Y.</li>
</ul>
<p><br></p>
</div>
<div
id="what-is-external-validity-and-why-can-it-be-low-for-lab-experiments-in-a-business-setting"
class="section level4">
<h4>(2) What is external validity, and why can it be low for lab
experiments in a business setting?</h4>
<ul>
<li><strong>Explanation</strong>: External validity is the extent to
which any findings in experiments can be extrapolated to other settings
outside a laboratory. However, lab experiments in a business setting
have low external validity as real-world environments are useful to
observe real customers’ behaviors which affected by diverse factors that
cannot be identically reflected in the lab experiments. Therefore,
results from the lab experiments cannot be extrapolated into real
scenarios accurately. In contrast, lab experiments in medical settings
can be more extrapolated well in other settings outside the laboratory
as medication effects are mostly related to biological mechanism in
body, which is almost similar to human beings.</li>
</ul>
<p><br></p>
</div>
<div
id="why-do-field-experiments-tend-to-have-higher-external-validity-than-laboratory-experiments-in-a-business-setting"
class="section level4">
<h4>(3) Why do field experiments tend to have higher external validity
than laboratory experiments in a business setting?</h4>
<ul>
<li><strong>Explanation</strong>: In a laboratory experiment, the
primary limitation is that the experiment cannot perfectly reflect a
real-scenario in which customers behave, which prevents external
validity because there is high possibility that customers behave
differently in the real-scenario. For example, in the laboratory
experiment, customers prefer to purchase products with lower prices
compared to field experiments where customers prefer to purchase
high-quality products with higher prices. In a field experiment, we can
monitor how customers behave in a real environment which would be
maintained when applying learnings from this experiment and this
identical environment can lead to similar results we have already
obtained in the experiment.</li>
</ul>
<p><br></p>
</div>
<div
id="when-it-comes-to-measuring-important-quantities-like-price-elasticity-research-has-compared-how-field-experiments-perform-relative-to-using-statistical-models-on-observational-non-experimental-data.-what-has-the-research-found-and-what-likely-explain-this-finding"
class="section level4">
<h4>(4) When it comes to measuring important quantities like price
elasticity, research has compared how field experiments perform relative
to using statistical models on observational (non-experimental) data.
What has the research found, and what likely explain this finding?</h4>
<ul>
<li><strong>Explanation</strong>: When examining price elasticity in a
field experiment, researcher can control other factors except for price
to see the true relationship between price and demand. The researcher
controls the quality of products, which could affect demand and set
different prices in different locations to identify the price effect on
the demand. However, in case of an observational data, researchers use
the dataset already gathered in which other factors (confounders),
affecting the price cannot be controlled. However, price effect can be
endogenous, meaning that other factors such as quality can affect the
price and demand, which prevents researchers from identifying the true
price effect on the demand. For example, even if the real effect of the
price on the demand is that higher price leads to lower demand, the
quality of product typically increase prices, which also makes the
demand higher. However, the observational data cannot remove this
confounding factor, which confounds the true price effect on the
demand.</li>
</ul>
<p><br></p>
</div>
<div
id="in-the-context-of-an-experiment-explain-what-is-the-p-value-tells-you-about-your-observed-sample-mean-y-."
class="section level4">
<h4>(5) In the context of an experiment, explain what is the p-value
tells you about your observed sample mean y ̂.</h4>
<ul>
<li><strong>Explanation</strong>: p-value is the probability of
observing extreme values which are not likely to be observed when
assuming the null hypothesis is true. Observing the extreme values
indicates that there are chances that the null hypothesis is false. For
example, in a one-tailed greater test, Let’s assume that the null
hypothesis is that the sample mean is less than or equal to 5 while the
alternative hypothesis is the sample mean is greater than 5. If the
p-value for observing an extreme value greater than 5 is 0.05 which is
less than 0.05 significance level, the probability of observing this
extreme value is too small not to reject the null hypothesis.</li>
</ul>
<p><br></p>
</div>
<div
id="sometimes-people-confuse-significance-level-α-with-the-likelihood-that-a-discovery-is-false.-explain-the-distinction-between-the-two."
class="section level4">
<h4>(6) Sometimes people confuse significance level α with the
likelihood that a discovery is false. Explain the distinction between
the two.</h4>
<ul>
<li><strong>Explanation</strong>: - The significance level of α is the
probability of rejecting the null hypothesis when it is actually true.
In other words, the significance level of α is the probability of
committing the type 1 error. When p-value which is the probability of
observing extreme values when assuming that the null hypothesis is true
is less than the significance level of α, the null hypothesis can be
rejected.</li>
<li>In case of the significance level of α, it assumes when the null
hypothesis is actually true and it indicates that probability that we
falsely conclude that a sample statistic is statistically significant.
However, a false discovery in a past experiment is the case that we
already concluded that a sample statistic is statistically significant
by comparing a p-value with a specific significance level but after
that, we found that the null hypothesis is true and there is not true
effect. The false discovery rate is typically much higher than the
significance level of α as there are diverse variables which led to a
false discovery in experiments such as repetitive experiments increase
the chances of obtaining the number of significant results by chance and
statistical power of the experiment being lower at that moment.</li>
</ul>
<p><br></p>
</div>
<div
id="what-two-factors-discussed-in-class-increase-statistical-power-of-an-experiment-one-of-these-you-can-control-one-of-them-you-cannot."
class="section level4">
<h4>(7) What two factors discussed in class increase statistical power
of an experiment? One of these you can control, one of them you
cannot.</h4>
<ul>
<li><strong>Explanation</strong>: We can increase the number of sample
size to increase statistical power of an experiment. However, we cannot
control the lower variation in the dependent variable Y. If Y is the
true value and Y ̂ is the expected value from SRF, the variation in the
dependent variable is Y - Y ̂ and if this difference is small, the
statistical power increases.</li>
</ul>
<p><br></p>
</div>
<div
id="in-business-experiments-we-sometimes-use-smaller-sample-sizes-that-do-not-have-the-statistical-power-to-detect-all-effects.-why-is-this-typically-ok"
class="section level4">
<h4>(8) In business experiments, we sometimes use smaller sample sizes
that do not have the statistical power to detect all effects. Why is
this typically ok?</h4>
<ul>
<li><strong>Explanation</strong>: If we do not have to detect all effect
but want to only focus on profitable effects, it is allowed to use
smaller sample sizes even if they cannot detect all effects compared to
larger sample sizes.</li>
</ul>
<p><br></p>
</div>
<div
id="why-do-we-care-about-incremental-conversion-rate-of-the-treatment-group-rather-than-the-total-click-through-rate"
class="section level4">
<h4>(9) Why do we care about incremental conversion rate of the
treatment group, rather than the total click through rate?</h4>
<ul>
<li><strong>Explanation</strong>: While Incremental conversion rate
considers both treatment group and control group (baseline), Click
through rate (CTR) also includes the number of people clicking a banner
without being influenced by the treatment, making it hard to distinguish
with the true effect of the treatment. The incremental conversion rate
compares the treatment effect with the baseline effect of the control
group, making it effective to evaluate the true treatment effect. If the
conversion rate increases compared to the baseline group, it indicates
the treatment effect is effective.</li>
</ul>
<p><br></p>
</div>
<div
id="for-an-online-experiment-where-we-are-showing-an-ad-with-the-intent-of-increasing-sales-on-our-website-we-will-typically-calculate-the-roi-of-ad.-explain-in-simple-terms-no-formula-needed-the-two-major-components-of-such-as-roi-calculation."
class="section level4">
<h4>(10) For an online experiment where we are showing an ad with the
intent of increasing sales on our website, we will typically calculate
the ROI of ad. Explain in simple terms – no formula needed – the two
major components of such as ROI calculation.</h4>
<ul>
<li><strong>Explanation</strong>: In an experiment for showing an
advertisement to increase sales, cost and revenue are two major
components for ROI calculation. The cost per incremental conversion
includes the advertisement cost contributed to increasing the number of
converted customers due to the advertisement itself. The revenue is the
total amount of money spent by converted customers. However, the net
revenue must be subtracted by other operational costs other than the
advertisement cost itself. Additionally, the revenue can include profit
obtained by converted customers from both treatment group and control
group. In more conservative way, the revenue only includes profit
occurred by converted customers in the treatment group.</li>
</ul>
<p><br></p>
</div>
<div
id="we-have-run-an-experiment-online-showing-an-ad-hoping-to-increase-our-sales.-the-ad-was-shown-to-500-people-and-another-400-people-were-tracked-as-a-control-group.-10-of-the-treatment-group-and-8-of-the-control-group-purchased-our-product.-the-treatment-group-generated-60-in-profit-not-including-cost-of-the-ad-while-the-control-group-generated-45-in-profit.-the-ad-cost-was-0.01-each-we-show-it.-our-roi-benchmark-is-30-should-we-continue-to-show-this-ad-explain-why-the-equations-below-may-be-helpful."
class="section level4">
<h4>(11) We have run an experiment online showing an ad, hoping to
increase our sales. The ad was shown to 500 people, and another 400
people were tracked as a control group. 10% of the treatment group and
8% of the control group purchased our product. The treatment group
generated $60 in profit (not including cost of the ad), while the
control group generated $45 in profit. The ad cost was $0.01 each we
show it. Our ROI benchmark is 30%; should we continue to show this ad?
Explain why? The equations below may be helpful.</h4>
<ul>
<li><strong>Explanation</strong>: 500 people were included in the
treatment group and 400 people were included in the control group. 50
customers purchased products after having seen the advertisement. In
contrast, 32 customers purchased products without seeing any
advertisement. The incremental conversion rate is 2% by deducting 8%
from 10%. $60 is a profit obtained by the treatment group after
deducting operational expenses other than the cost of the advertisement
and $45 is a profit obtained by the control group after subtracting
operational expenses other than the cost of the advertisement. The
advertisement cost is $0.01 * 500 = $5 and our ROI benchmark is 30%,
which means to continue the advertisement, the ROI should be at least
over 30%.</li>
<li>First of all, Cost per Increment Conversion (CPIC) is $5 / (2% *
500) = $0.5. Increment conversion means additional conversion amount due
to the true effect of the advertisement itself, which is the difference
between the treatment group and the control group. The cost is assigned
for the true incremental conversion amount compared to the control
group.</li>
<li>Secondly, profit for increment conversion is $60 / (500 * 10%) =
$1.2. However, this is the profit for conversions in the treatment
effect and not the profit for additional incremental conversion compared
to the control group. In less conservative way, the profit for
conversions in the control group can be added, and the total profit is
$60 + $45 / (500<em>10%) + (400</em>8%) = $1.28.</li>
<li>In more conservative way, ROI = (1.2 – 0.5) / 0.5 * 100 = 140% weigh
over the benchmark. In conclusion, this outcome leads to a data-driven
decision making to continue the advertisement.</li>
</ul>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="experiments-2" class="section level3">
<h3>2. Experiments 2</h3>
<p><br></p>
<div id="explain-what-anova-tests." class="section level4">
<h4>(1) Explain What ANOVA tests.</h4>
<ul>
<li><strong>Explanation</strong>: When comparing multiple group means,
ANOVA test is used to determine whether all means among multiple groups
are the same as each other or at least two means differ. In one-way
ANOVA, there should be multiple k treatments and the null hypothesis is
all treatment means are the same, indicating that there are no
differences among them. Otherwise, the alternative hypothesis is at
least two means differ, indicating that there is a treatment different
from any of other treatments.</li>
</ul>
<p><br></p>
</div>
<div
id="after-running-anova-we-may-run-a-tukey-multiple-comparisons-test.-what-does-this-test-tell-us"
class="section level4">
<h4>(2) After running ANOVA, we may run a Tukey Multiple Comparisons
test. What does this test tell us?</h4>
<ul>
<li><strong>Explanation</strong>: The ANOVA test is an omni-test,
meaning that we can identify at least two means differ after rejecting
the null hypothesis, while we cannot identify which treatments differ
from each other. The Tukey Multiple Comparisons test helps identify
which treatment group differs from any of other treatment groups.</li>
</ul>
<p><br></p>
</div>
<div
id="after-running-anova-we-may-run-a-tukey-multiple-comparisons-test.-what-does-this-test-tell-us-1"
class="section level4">
<h4>(3) After running ANOVA, we may run a Tukey Multiple Comparisons
test. What does this test tell us?</h4>
<ul>
<li><strong>Explanation</strong>: There are 3 major Post hoc tests to
identify which treatment groups differ among others, such as Fisher’s
Least Significant Difference (LSD), Bonferroni correction, and Tukey’s
multiple comparisons. In case of Fisher’s LSD, it inflates the
probability of committing at least one type 1 error when comparing
multiple pairwise comparisons. If the number of pairwise comparisons
increases, the probability of committing at least one type 1 error also
increases because the Fisher’s LSD applied 0.05 significance level for
each pairwise comparison.</li>
<li>To address this inflation, Bonferroni is used by dividing the
significance level for each pairwise comparison by the number of
comparisons to prevent the inflation of the probability of at least one
type 1 error. However, it increases the probability of type 2 error due
to the lower significance level α. P-value must be lower than the
extremely lower significance level, which increases the type 2 error by
missing the true effect.</li>
<li>In case of a Tukey’s multiple comparison, it uses family-wise error
rate which is the same as significance level of α. It controls the
probability of committing at least one type 1 error across all multiple
comparisons in the test as around significance level of 0.05 to reduce
the inflation of type 1 error by adjusting p-values. When p-value
adjusted, it reflects that the observed values are less extreme under
the null hypothesis, reducing the likelihood of falsely rejecting the
null hypothesis. This adjustment helps to maintain the integrity of the
test by preventing the rejection of the null hypothesis when it is
actually true, thereby controlling for Type 1 error across multiple
comparisons.</li>
</ul>
<p><br></p>
</div>
<div
id="why-do-we-sometimes-use-a-bonferroni-correction-in-our-regressions"
class="section level4">
<h4>(4) Why do we sometimes use a Bonferroni correction in our
regressions?</h4>
<ul>
<li><strong>Explanation</strong>: Bonferroni is used by dividing the
significance level for each pairwise comparison by the number of
comparisons to prevent the inflation of the probability of at least one
type 1 error. However, it increases the probability of type 2 error due
to the lower significance level α. P-value must be lower the extremely
lower significance level, which increases the type 2 error by missing
the true effect.</li>
</ul>
<p><br></p>
</div>
<div
id="broadly-speaking-when-would-we-run-planned-comparison-tests-instead-of-a-tukey-multiple-comparison-test-and-what-would-be-the-downside-of-running-the-tukey-multiple-comparison-instead"
class="section level4">
<h4>(5) Broadly speaking, when would we run planned comparison tests
instead of a Tukey Multiple Comparison test? And what would be the
downside of running the Tukey Multiple Comparison instead?</h4>
<ul>
<li><strong>Explanation</strong>: The Tukey Multiple Comparison test
compares all possible comparisons which inflates the probability of
committing at least one type 1 error. However, the planned comparison
test only compares some comparisons interested in, reducing the
probability of at least one type 1 error. Matching our tests to our
hypotheses, rather than testing all possible combinations of conditions,
reduces the risk of Type 1 error; running a Tukey test instead would
have a higher likelihood of Type1 error, simply because we are running
more tests, and would necessitate more aggressively adjusting our
p-values. Consequently, the p-values of the tests actually appropriate
for our hypotheses would be higher than they should be if we use a Tukey
test.</li>
</ul>
<p><br></p>
</div>
<div
id="explain-why-we-can-estimate-a-treatment-effect-in-settings-with-no-control-group-if-we-observe-different-levels-of-treatment-e.g.-treatment-doses"
class="section level4">
<h4>(6) Explain why we can estimate a treatment effect in settings with
no control group if we observe different levels of treatment (e.g.,
treatment doses)</h4>
<ul>
<li><strong>Explanation</strong>: Even if there is no control group,
treatment doses set different levels or intensity of treatment and
determine if there is any different effect from others on the dependent
variable of Y. For example, if we want to determine if different levels
of medication intake influence a disease, we can set different levels of
medication intake and observe the treatment effect of each level to
decide whether there is any difference among them.</li>
</ul>
<p><br></p>
</div>
<div id="interpret-α-and-β-in-the-following-regression"
class="section level4">
<h4>(7) Interpret α and β in the following regression:</h4>
<p><span class="math display">\[
Y = \alpha + \beta \exp(X)
\]</span> - <strong>Explanation</strong>: When exp⁡(X) is zero, α means
the baseline effect of the dependent variable Y. When exp⁡(X) increases
by 1 unit, the dependent variable Y increases by β.</p>
<p><br></p>
</div>
<div
id="when-modeling-treatment-doses-in-a-regression-we-have-the-option-of-taking-a-parametric-or-non-parametric-approach.-explain-the-pros-and-cons-of-choosing-a-parametric-approach-over-a-non-parametric-approach-or-vice-versa."
class="section level4">
<h4>(8) When modeling treatment doses in a regression, we have the
option of taking a parametric or non-parametric approach. Explain the
pros and cons of choosing a parametric approach over a non-parametric
approach (or vice-versa).</h4>
<ul>
<li><strong>Explanation</strong>: A non-parametric approach represents
the relationship between X and Y variables without exact mathematical
equations. If we want to determine the price elasticity and identify the
relationship between price and demand, non-parametric describes
relationships based on observed data without using mathematical
equations. However, a parametric approach uses a specific mathematical
equation such as linear or polynomial and predicts the dependent
variable Y with independent variables. This parametric approach lies
strengths in predicting Y values outside the range of X values, which is
known as extrapolation. This strength depends on the accuracy of the
chosen model and its underlying assumptions. In contrast, non-parametric
approach cannot reliably predict the Y value outside the range of the X
values as it does not generalize beyond the available data.
Additionally, non-parametric test can have an over-fitting issue while
making random noise as meaningful. In case of parametric test, it may
result in error if the chosen model does not accurately represent the
data.</li>
</ul>
<p><br></p>
<p><br><br><br></p>
</div>
</div>
<div id="experiments-3" class="section level3">
<h3>3. Experiments 3</h3>
<p><br></p>
<p>c We are evaluating a treatment – the option to switch to a
prize-linked savings account – on a user’s savings. An experiment was
run in which bank customers were randomly assigned to a treatment or
control group. We believe ‘old’ users and ‘young’ users may differ in
their treatment response, and so have constructed the following
regression equation to evaluate our hypothesis with our data. I[OLD] is
a dummy variable equal to 1 if a user is ‘old’, and I[TREAT] is a dummy
variable equal to 1 if a user is in the treatment group. Interpret the
coefficients in this regression (below).</p>
<p><span class="math display">\[
\text{Savings}_i = \alpha_1 + \alpha_2 I[\text{OLD}]_i + \beta_1
I[\text{TREAT}]_i + \beta_2 \big(I[\text{TREAT}]_i \cdot
I[\text{OLD}]_i\big)
\]</span> - <strong>Explanation</strong>: First of all, α_1 refers to
the young age in the control group. α_2 refers to the difference between
the young and old in the control group. β_1 refers to the treatment
effect for the young age. It indicates the difference between the young
age in the control group and the treatment group. β_2 refers to the
difference of the treatment effect between the young and old. It
indicates both ages are in each treatment group and determine whether
there is any difference in the treatment effect for those groups.</p>
<p><br></p>
<div
id="we-are-evaluating-a-treatment-the-option-to-switch-to-a-prize-linked-savings-account-on-a-users-savings.-an-experiment-was-run-in-which-bank-customers-were-randomly-assigned-to-a-treatment-or-control-group.-treatment-response-may-vary-by-age-and-income.-we-have-segment-customers-into-two-age-groups-35-and-35-and-three-income-groups-less-than-50k-between-50k-and-100k-and-greater-than-100k.-the-variable-age35-is-a-dummy-variable-indicating-that-a-user-is-35-or-older.-using-the-regression-output-below-forecast-the-savings-for-each-type-of-user-in-the-table-below."
class="section level4">
<h4>(2) We are evaluating a treatment – the option to switch to a
prize-linked savings account – on a user’s savings. An experiment was
run in which bank customers were randomly assigned to a treatment or
control group. Treatment response may vary by age and income. We have
segment customers into two age groups (&lt;35 and ≥35) and three income
groups (less than $50K, between $50K and $100K, and greater than $100K).
The variable Age35 is a dummy variable indicating that a user is 35 or
older. Using the regression output below, forecast the savings for each
type of user in the table below.</h4>
<p align="center">
<img src="images/Result.png" style="width:80%; height:auto;" align="center">
</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>
Table 1: Treatment Effects by User Type
</caption>
<thead>
<tr>
<th style="text-align:left;font-weight: bold;color: white !important;background-color: black !important;">
User.Type
</th>
<th style="text-align:right;font-weight: bold;color: white !important;background-color: black !important;">
Control.Group.Savings….
</th>
<th style="text-align:right;font-weight: bold;color: white !important;background-color: black !important;">
Treatment.Group.Savings….
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;color: blue !important;">
Income of $75K, Age 40
</td>
<td style="text-align:right;color: blue !important;">
1813.45
</td>
<td style="text-align:right;color: blue !important;">
2522.53
</td>
</tr>
<tr>
<td style="text-align:left;color: blue !important;">
Income of $125K, Age 25
</td>
<td style="text-align:right;color: blue !important;">
1387.26
</td>
<td style="text-align:right;color: blue !important;">
1279.72
</td>
</tr>
<tr>
<td style="text-align:left;color: blue !important;">
Income of $45K, Age 30
</td>
<td style="text-align:right;color: blue !important;">
597.29
</td>
<td style="text-align:right;color: blue !important;">
910.94
</td>
</tr>
</tbody>
</table>
<p><br></p>
</div>
<div
id="we-believe-high--and-low-income-users-may-differ-in-their-response-to-treatment-and-so-have-constructed-the-following-regression-equation-to-evaluate-our-hypothesis-with-our-data.-ihi-is-a-dummy-variable-equal-to-1-if-a-user-is-high-income-and-itreat-is-a-dummy-variable-equal-to-1-if-a-user-is-in-the-treatment-group.-the-savings-of-each-group-is-listed-in-the-table-below.-using-the-table-calculate-the-value-of-each-parameter-or-coefficient."
class="section level4">
<h4>(3) We believe high- and low-income users may differ in their
response to treatment, and so have constructed the following regression
equation to evaluate our hypothesis with our data. I[HI] is a dummy
variable equal to 1 if a user is ‘high income’ and I[TREAT] is a dummy
variable equal to 1 if a user is in the treatment group. The savings of
each group is listed in the table below. Using the table, calculate the
value of each parameter (or coefficient).</h4>
<p><span class="math display">\[
\text{Savings}_i = \alpha_1 + \alpha_2 I[\text{HI}]_i + \beta_1
I[\text{TREAT}]_i + \beta_2 \big(I[\text{TREAT}]_i \cdot
I[\text{HI}]_i\big)
\]</span></p>
<p align="center">
<img src="images/Result2.png" style="width:80%; height:auto;" align="center">
</p>
<ul>
<li><strong>Explanation</strong>: α_1 refers to the low-income in the
control group ($1,200) and $400 is β_1 which is the treatment effect of
the low-income group. α_2 is $800 which is the difference between the
low-income and high-income in the control group. β_2 is -$400 which is
the difference in the treatment effect between the low-income and
high-income group. In the low-income group, the treatment effect is
$400, while the treatment effect in the high-income group is $0. β_2
refers to the difference in the treatment effect of the high-income
group compared to the low-income group.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="experiments-4" class="section level3">
<h3>4. Experiments 4</h3>
<p>Causal Inference with Observational Data;
Difference-in-Differences</p>
<div id="in-simple-terms-explain-endogeneity." class="section level4">
<h4>(1) In simple terms, explain endogeneity.</h4>
<ul>
<li><strong>Explanation</strong>: Endogeneity occurs when the
independent variables are correlated with the error term. To prevent a
coefficient bias, the independent variables are independent from the
error term. However, omitted variables included in the error term are
correlated with independent variables which lead to coefficient bias,
making it hard to distinguish the true effect between the dependent and
independent variables. For example, if we want to identify the
relationship between educational level and income, we have to control
personal characteristics such as motivation and personal ability as they
are correlated with educational level, leading to the coefficient bias.
People with good personal ability often has higher educational level and
this variable also influence the dependent variable, making it hard to
identify the true effect between the income level and educational
level.</li>
</ul>
<p><br></p>
</div>
<div
id="we-want-to-understand-the-causal-effect-of-a-change-in-x-on-y.-say-there-are-two-time-periods-t_1-and-t_2-and-x-changed-in-the-second-period.-we-should-not-simply-compare-y-before-and-after-the-change-we-should-not-compare-y-in-t_2-to-y-in-t_1.-if-it-were-possible-what-counterfactual-would-we-really-want-to-compare-the-value-of-y-in-t_2-to"
class="section level4">
<h4>(2) We want to understand the causal effect of a change in X on Y.
Say there are two time periods, t_1 and t_2, and X changed in the second
period. We should not simply compare Y before and after the change; we
should not compare Y in t_2, to Y in t_1. If it were possible, what
counterfactual would we really want to compare the value of Y in t_2
to?</h4>
<ul>
<li><strong>Explanation</strong>: We cannot simply compare the dependent
variable Y during t_1 period and t_2 period. Otherwise, we can compare
the value of the dependent variable Y in t_2 period to the dependent
variable Y during the same period under the assumption if they had not
received any treatment. The counterfactual should be what would have
happened in the dependent variable Y if it had not received any
treatment effect during t_2 period. The counterfactual is the process of
assuming what would have happened to the dependent variable if the
independent variable had not been treated and compare this value to the
dependent variable obtained from the treated independent variable during
the same period in order to distinguish the true treatment effect during
the same period.</li>
</ul>
<p><br></p>
</div>
<div
id="what-features-of-panel-data-allow-us-to-conduct-a-difference-in-differences-analysis"
class="section level4">
<h4>(3) What features of panel data allow us to conduct a
difference-in-differences analysis?</h4>
<ul>
<li><strong>Explanation</strong>: A panel data has multiple observations
for multiple units across time. If some units are treated during a
specific period, the other units without treatment can be used to
identify the true treatment effect during the same period by acting as
counterfactual. For example, if there is treatment effect occurred from
July during one year for some units and parallel trends assumption is
not violated before the treatment effect, we can identify the treatment
effect by comparing the value for the treated units from July to
December to the value for the untreated units during the same period.
These untreated units during the same period (Jul – Dec) works as a
counterfactual for the treated units. We cannot simply compare the
dependent variable from Jan to Jun to the value from Jul to Dec to
acquire the treatment effect. We should assume what would have happened
if the treated units had not been treated by using the other units as
counterfactual and compare them during the same treatment period (Jul –
Dec).</li>
</ul>
<p><br></p>
</div>
<div
id="explain-the-regression-you-would-run-to-formally-test-the-parallel-trends-assumption-for-a-simple-difference-in-difference-model-how-you-would-interpret-the-results-and-how-the-results-would-determine-whether-the-assumption-is-violated.-to-help-your-exposition-you-can-assume-your-data-has-twelve-months-and-treatment-began-in-month-7."
class="section level4">
<h4>(4) Explain the regression you would run to formally test the
parallel trends assumption for a simple difference-in-difference model,
how you would interpret the results, and how the results would determine
whether the assumption is violated. To help your exposition, you can
assume your data has twelve months, and treatment began in month 7.</h4>
<p><span class="math display">\[
Y_{it} = \alpha + \delta \cdot \text{TreatmentGroup}_i + \sum_{t=2}^6
\gamma_t \cdot \text{MonthDummy}_t + \sum_{t=2}^6 \beta_t \cdot
(\text{TreatmentGroup}_i \cdot \text{MonthDummy}_t) + \epsilon_{it}
\]</span></p>
<ul>
<li><strong>Null Hypothesis (<span
class="math inline">\(H_0\)</span>):</strong></li>
</ul>
<p><span class="math display">\[
\beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6
\]</span></p>
<p>The null hypothesis states that there are no differences in trends
between the treatment and control groups before the treatment period,
meaning the parallel trends assumption holds.</p>
<ul>
<li><strong>Alternative Hypothesis (<span
class="math inline">\(H_1\)</span>):</strong></li>
</ul>
<p><span class="math display">\[
\text{At least one } \beta_t \neq 0 \text{ for } t = 2, 3, 4, 5, 6
\]</span></p>
<ul>
<li><strong>Explanation</strong>: The baseline month is January and
compare the control group and treatment group before the treatment
effect occurs. The beta coefficient refers to the difference between the
control group and the treatment group across months from Feb to Jun
before the treatment begins. If the null hypothesis is not rejected and
the beta coefficient is not statistically significant, it indicates the
parallel trends assumption between the control group and the treatment
group is maintained, which means there are no big difference between the
treatment group and the control group across time. If the parallel
trends assumption holds, any observed differences between the treatment
and control groups after the treatment occurs can be attributed to the
treatment effect, rather than pre-existing differences in trends. This
results indicates if there is any difference between them during the
post-treatment periods, this difference represents the true treatment
effect.</li>
</ul>
<p><br></p>
</div>
<div
id="explain-why-we-can-estimate-a-treatment-effect-when-all-units-receive-treatment-as-long-as-they-receive-treatment-at-different-times."
class="section level4">
<h4>(5) Explain why we can estimate a treatment effect when all units
receive treatment, as long as they receive treatment at different
times.</h4>
<ul>
<li><strong>Explanation</strong>: If some units receive a treatment
effect at different times, we can use the other units without receiving
any treatment yet as a control group for some units already received the
treatment effect.</li>
</ul>
<p><br></p>
</div>
<div
id="the-standard-difference-in-difference-equation-for-a-treatment-group-control-group-treatment-period-and-control-period-are-below.-explain-what-each-of-the-parameters-or-coefficients-measures"
class="section level4">
<h4>(6) The standard difference in difference equation for a treatment
group, control group, treatment period, and control period are below.
Explain what each of the parameters (or coefficients) measures?</h4>
<p><span class="math display">\[
Y = \alpha + \beta \cdot I[\text{TrtPer}] + \gamma \cdot
I[\text{TrtGroup}] + \delta \cdot (I[\text{TrtPer}] \cdot
I[\text{TrtGroup}])
\]</span></p>
<ul>
<li><strong>Explanation</strong>: α measures the dependent variable Y of
interest during the pre-treatment period for a control group. β measures
the time effect for the control group during the treatment period. Even
if the control group is not treated, the time effect between the control
period and treatment period based on its value. γ measures the
difference between the control group and the treatment group during the
pre-treatment period. δ measures the difference of the treatment effect
between the control group and the treatment group. It measures the
difference between the treatment effect in the treatment group and the
time effect occurred in the control group.</li>
</ul>
<p><br></p>
</div>
<div
id="the-standard-difference-in-difference-equation-for-a-treatment-group-control-group-treatment-period-and-control-period-are-below.-corresponding-data-is-also-in-a-table-below.-calculate-the-value-of-each-parameter."
class="section level4">
<h4>(7) The standard difference in difference equation for a treatment
group, control group, treatment period, and control period are below.
Corresponding data is also in a table below. Calculate the value of each
parameter.</h4>
<p><span class="math display">\[
Y = \alpha + \beta \cdot I[\text{TrtPer}] + \gamma \cdot
I[\text{TrtGroup}] + \delta \cdot (I[\text{TrtPer}] \cdot
I[\text{TrtGroup}])
\]</span></p>
<p align="center">
<img src="images/Result3.png" style="width:80%; height:auto;" align="center">
</p>
<ul>
<li><strong>Explanation</strong>: In the above table, α is $350 which
indicates the dependent variable Y of interest during the control
period. β = 90 which indicates the time effect between the control
period and the treatment period in the control group. γ = -10 which
indicates the difference between the control group and the treatment
group during the pre-treatment period. δ = -40, which indicates the
difference between the time effect and the treatment effect during the
treatment period. The treatment effect in the treatment group during the
treatment period is smaller than the time effect applied for the control
group.</li>
</ul>
<p><br></p>
</div>
<div
id="we-are-modeling-data-from-a-staggered-treatment-setting-using-a-staggered-difference-in-difference-or-two-way-fixed-effects-model.-assume-the-data-come-from-multiple-states-and-multiple-years.-our-treatment-of-interest-is-the-change-in-tuition-at-universities-of-each-state-from-year-to-year.-we-wish-to-measure-the-impact-of-tuition-changes-on-some-variable-y-and-will-include-tuition-as-a-continuous-variable.-what-other-variables-are-needed-to-complete-the-model-what-will-those-variables-control-for"
class="section level4">
<h4>(8) We are modeling data from a staggered treatment setting, using a
staggered difference-in-difference (or “two-way-fixed-effects”) model.
Assume the data come from multiple states and multiple years. Our
treatment of interest is the change in tuition at universities of each
state from year to year. We wish to measure the impact of tuition
changes on some variable Y, and will include tuition as a continuous
variable. What other variables are needed to complete the model? What
will those variables control for?</h4>
<ul>
<li><strong>Explanation</strong>: We need two-fixed-effects such as
fixed year variable and fixed state variable. In case of fixed year
variable, it includes a specific event such as COVID-19, affecting both
tuition fee and the dependent variable Y of interest. This impact must
be controlled for using year fixed-effects to account for differences
across different years. Additionally, in case of fixed state variable,
states have their own characteristics which can influence both the
tuition fee and the dependent variable. To capture the true effect of
the tuition fee on the dependent variable Y, this state influence must
be accounted for by including state fixed-effects in the model.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="matching-synthetic-control-and-instrumental-variables"
class="section level3">
<h3>5. Matching, Synthetic Control, and Instrumental Variables</h3>
<div
id="say-that-we-have-data-on-some-consumers-that-received-treatment-and-other-consumers-that-did-not.-the-demographic-makeup-of-treated-consumers-differs-from-non-treated-consumers.-we-find-that-our-measure-of-interest-in-y-is-much-larger-among-treated-consumers-than-non-treated-consumers.-why-might-this-apparently-strong-treatment-effect-be-misleading-in-this-setting"
class="section level4">
<h4>(1) Say that we have data on some consumers that received treatment
and other consumers that did not. The demographic makeup of treated
consumers differs from non-treated consumers. We find that our measure
of interest in Y is much larger among treated consumers than non-treated
consumers. Why might this apparently strong treatment effect be
misleading in this setting?</h4>
<ul>
<li><strong>Explanation</strong>: Differences between the control group
and the treatment group such as age, income, educational level and other
demographic factors could influence the dependent variable Y of
interest, misleading the true treatment effect. To address this, we have
to match the demographic makeup between the treatment group and the
control group to remove the effect of demographic factors from the
dependent variable and identify the true treatment effect.</li>
</ul>
<p><br></p>
</div>
<div
id="when-can-matching-be-helpful-in-addressing-endogeneity-when-can-it-not"
class="section level4">
<h4>(2) When can matching be helpful in addressing endogeneity? When can
it not?</h4>
<ul>
<li><strong>Explanation</strong>: Endogeneity is related to omitted
variables included in the error term and this error term is correlated
with the independent variable X of interest. Matching the treatment
group with the control group based on observable confounding variables
helps identify the true treatment effect. However, unobservable
confounding variables can also influence both independent and dependent
variable. The problem is that these variables are unobservable not able
to remove the impact from the regression model by matching them between
the groups.</li>
</ul>
<p><br></p>
</div>
<div
id="when-can-matching-be-helpful-in-addressing-a-parallel-trends-violation"
class="section level4">
<h4>(3) When can matching be helpful in addressing a parallel trends
violation?</h4>
<ul>
<li><strong>Explanation</strong>: Matching can be helpful when a
parallel trends violation is due to the difference in observable
confounding variables between the treatment and control group. If
demographic makeup such as age, income, educational levels which are
observable are different between the two groups, matching is helpful to
make those factors consistent which can address a parallel trends
violation due to observable confounding variables and help identify the
treatment effect of interest.</li>
</ul>
<p><br></p>
</div>
<div
id="coarsened-exact-matching-effectively-provides-us-with-a-weighted-sample.-what-is-the-objective-of-these-weights"
class="section level4">
<h4>(4) Coarsened Exact Matching effectively provides us with a weighted
sample. What is the objective of these weights?</h4>
<ul>
<li><strong>Explanation</strong>: Weights can be used to match a
treatment group with a control group if they are different in observable
confounding variables such as age. For example, the proportion of each
age group in the treatment group is different to the control group,
weights can be used to balance the proportion of each age group between
the two to prevent this variable to confound the true treatment effect
on the dependent variable Y.</li>
</ul>
<p><br></p>
</div>
<div
id="describe-what-a-synthetic-control-group-is.-explain-simply-how-and-why-we-would-construct-one."
class="section level4">
<h4>(5) Describe what a synthetic control group is. Explain (simply) how
and why we would construct one.</h4>
<ul>
<li><strong>Explanation</strong>: A synthetic control group constructed
group combining units matched with the treatment group of interest based
on observable confounding variables and the change trends on the
dependent variable Y prior to the treatment period. We can use weights
to make the control group comparable with the treatment group to control
for the effects of observable confounding variables while simultaneously
considering the trends of the dependent variables matched with the one
shown on the treatment group during the control period. This method
considers both matching the observable variables and the dependent
variable with the treatment group when gathering units for the control
group to comply with the standard difference-in-difference parallel
trends assumption.</li>
<li>If the trends on the dependent variable Y for the control group is
matched with the treatment group except for the confounding variable
matches, it can suggest that any change occurred during the treatment
period is related to the treatment effect and help identify the true
causal treatment effect on the dependent variable.</li>
</ul>
<p><br></p>
</div>
<div
id="what-are-instruments-and-how-do-they-help-us-with-endogenous-variables"
class="section level4">
<h4>(6) What are instruments and how do they help us with endogenous
variables?</h4>
<ul>
<li><strong>Explanation</strong>: Instrument variables are tools used to
address endogeneity in regression analysis. Endogeneity occurs when the
independent variable is correlated with the error term. Independent
variables are correlated with error terms due to omitted confounding
variables which influence both the independent variable and the
dependent variable. These confounding variables make it hard to
distinguish the true causal relationship between the independent
variable and the dependent variable. However, Instrument variables which
are not correlated with error term and does not affect the dependent
variable except through the independent variable. By using the variation
in the independent variable explained by the instrument (exogeneity),
IVs help estimate the causal effect of the independent variable on the
dependent variable, free from the biases introduced by endogeneity.</li>
</ul>
<p><br></p>
</div>
<div
id="what-are-the-three-assumptions-that-must-be-satisfied-for-instrumental-variables-that-we-studied-in-class-explain-the-assumptions."
class="section level4">
<h4>(7) What are the three assumptions that must be satisfied for
instrumental variables that we studied in class? Explain the
assumptions.</h4>
<ul>
<li><strong>Explanation</strong>: Firstly, inclusion must be satisfied.
Instrumental variables affect independent variable of interest. Second,
exclusion must be satisfied. Instrumental variables affect the dependent
variable only through the independent variable. Lastly, independence
must be satisfied. Instrumental variables must not be correlated with
error terms especially with omitted confounding variables which can
affect the dependent variable directly.</li>
</ul>
<p><br></p>
</div>
<div
id="describe-the-two-stage-least-squares-procedure-in-terms-of-regression-models-and-the-variables-x-y-and-z-and-explain-what-the-approach-accomplishes"
class="section level4">
<h4>(8) Describe the two-stage least squares procedure in terms of
regression models and the variables X, Y, and Z, and explain what the
approach accomplishes?</h4>
<ul>
<li><strong>Explanation</strong>: First, to isolate exogenous variation
in X using Z, the 2SLS model regresses X on Z. For example, if the
independent variable is educational level and the dependent variable Y
is income, instrumental variable can be the distance between the school
and students’ housing. The distance can affect people’s educational
level and the first step of regression isolates the variation in the
independent variable, educational level, that is explained by the
instrumental variable, distance.</li>
<li>Second, if this effect can be denoted by X ̂, the dependent variable
Y is regressed on this exogeneous variation. The 2SLS is free from the
effects of confounding variables which can affect both the independent
variable and dependent variable, making it hard to distinguish the true
educational effect on the income level. This model only isolates the
variation in the independent variable X occurred by the instrument
variable and regresses the dependent variable Y on the exogenous effect
reflected on those variation to identify the true causal
relationship.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="customer-analytics" class="section level3">
<h3>6. Customer Analytics</h3>
<div id="what-is-price-discrimination" class="section level4">
<h4>(1) What is price discrimination?</h4>
<ul>
<li><strong>Explanation</strong>: Price discrimination offers different
prices for customers for the same services or products.</li>
</ul>
<p><br></p>
</div>
<div id="how-can-dynamic-pricing-help-smooth-demand-and-supply"
class="section level4">
<h4>(2) How can dynamic pricing help smooth demand and supply?</h4>
<ul>
<li><strong>Explanation</strong>: Dynamic pricing offers different
prices by time such as different pricing by time of day or day of week.
If prices go up in a specific time, it suppresses demand whereas if
prices go down during a specific period, it increases demand. Dynamic
pricing can lead to price discrimination as this strategy offers
different prices during a specific period for the same products or
services. In case of supply, if prices go up such as for car sharing
services, it would motivate drivers to work, leading to the increase in
supply.</li>
</ul>
<p><br></p>
</div>
<div
id="what-are-the-benefits-of-surge-pricing-to-the-riders-who-must-pay-higher-prices"
class="section level4">
<h4>(3) What are the benefits of surge pricing to the riders who must
pay higher prices?</h4>
<ul>
<li><strong>Explanation</strong>: If prices go up, in case of
car-sharing services, the riders can easily catch rides even if they
must pay higher prices as drivers are motivated to supply drives for the
riders during the surge pricing time.</li>
</ul>
<p><br></p>
</div>
<div id="why-is-dynamic-pricing-likely-to-lead-to-price-discrimination"
class="section level4">
<h4>(4) Why is dynamic pricing likely to lead to price
discrimination?</h4>
<ul>
<li><strong>Explanation</strong>: Offering discounts during certain
period to attract more customers is a dynamic pricing, which can lead to
price discrimination as customers purchase a product during discount
seasons pay less compared to customers purchasing the same product
during normal seasons, related to price discrimination for the same
product or services.</li>
</ul>
<p><br></p>
</div>
<div
id="why-do-temporary-discounts-lead-to-what-is-commonly-referred-to-as-the-post-promotion-dip"
class="section level4">
<h4>(5) Why do temporary discounts lead to what is commonly referred to
as the “post-promotion dip”?</h4>
<ul>
<li><strong>Explanation</strong>: During a temporary promotion season,
customers are attracted to purchase more units of a product compare to
the actual amount they need for stockpiling. After the promotion,
post-promotion dip happens as customers already purchased more units of
the product as needed, leading to the reduction in the sales.</li>
</ul>
<p><br></p>
</div>
<div id="what-are-the-pros-and-cons-of-temporary-discounts"
class="section level4">
<h4>(6) What are the pros and cons of temporary discounts?</h4>
<ul>
<li><strong>Explanation</strong>: The pros of temporary discounts
attract customers who would not have purchased any products at full
price by motivating them to purchase the product with more reasonable
price. The cons of the temporary discounts offer discounts to customers
who would have purchased the product without any discount, meaning that
a company offering this discount offers unnecessary discounts for
customers who did not need any discount for purchasing the product.</li>
</ul>
<p><br></p>
</div>
<div
id="we-want-to-model-the-impact-of-stockpiling-on-our-unit-sales-of-toilet-paper.-stockpiling-typically-occurs-when-there-are-price-discounts-and-the-impact-on-sales-lasts-from-some-period-of-time.-as-a-result-we-collect-data-on-prices-and-sales-at-our-store-and-create-the-regression-equation-below.-interpret-the-coefficients.-would-you-anticipate-the-coefficients-of-γ-would-be-positive-or-negative-and-why"
class="section level4">
<h4>(7) We want to model the impact of stockpiling on our unit sales of
toilet paper. Stockpiling typically occurs when there are price
discounts, and the impact on sales lasts from some period of time. As a
result, we collect data on prices and sales at our store and create the
regression equation below. Interpret the coefficients. Would you
anticipate the coefficients of γ would be positive or negative, and
why?</h4>
<p><span class="math display">\[
\text{Sales}_t = \alpha + \beta \cdot \text{Price}_t + \gamma_1 \cdot
\text{Price}_{t-1} + \gamma_2 \cdot \text{Price}_{t-2} + \gamma_3 \cdot
\text{Price}_{t-3} + \gamma_4 \cdot \text{Price}_{t-4}
\]</span></p>
<ul>
<li><strong>Explanation</strong>: α measures the sales when all prices
are zero. β measures the sales of the current week or today. All γ
measure the impact of discounts during 4 weeks on the sales for this
week. If there are discounts on the units of toilet paper prior to this
week, the sales during this week or today is likely to decrease due to
stockpiling during the promotions. The coefficients of γ would be
positive because if the prices during the promotion weeks drop, the
sales for this week are likely to drop, moving the same direction with
the coefficients of γ.</li>
</ul>
<p><br></p>
</div>
<div
id="using-the-same-regression-from-the-prior-question-if-we-believe-that-stockpiling-hurts-sales-for-three-weeks-and-then-sales-return-to-normal.-if-our-hypothesis-is-correct-describe-what-the-sign-and-significance-whether-it-would-be-significant-or-not-of-each-γ-coefficient-would-be."
class="section level4">
<h4>(8) Using the same regression from the prior question, if we believe
that stockpiling hurts sales for three weeks and then sales return to
normal. If our hypothesis is correct, describe what the sign and
significance (whether it would be significant or not) of each γ
coefficient would be.</h4>
<ul>
<li><strong>Explanation</strong>: If stockpiling due to promotions hurts
sales for three weeks, γ for three weeks prior to this week would be
positive and statistically significant whereas γ for 4 weeks prior to
this week would not be significant as stockpiling hurts sales for this
week only for three weeks.</li>
</ul>
<p><br></p>
</div>
</div>
<div id="recommendation-systems" class="section level3">
<h3>7. Recommendation Systems</h3>
<div
id="i-claim-that-recommendation-systems-like-the-collaborative-filter-help-us-account-for-unobservable-heterogeneous-preferences.-what-do-i-mean-by-that"
class="section level4">
<h4>(1) I claim that recommendation systems like the collaborative
filter help us account for unobservable heterogeneous preferences. What
do I mean by that?</h4>
<ul>
<li><strong>Explanation</strong>: Unobservable heterogeneous preferences
cannot accurately be analyzed but the collaborative filter helps
recommend suitable products or services for users by referencing other
users’ choices with similar characteristics. We cannot accurately know
heterogeneous preferences for the target user but can assume other users
with similar characteristics with the target user provides sufficient
hints for the target user’s preferences and recommend products or
services for the target user that were recommended for other similar
users with similar preferences.</li>
</ul>
</div>
<div
id="explain-at-least-two-benefits-of-recommendation-systems-to-customers-that-we-discussed-in-class."
class="section level4">
<h4>(2) Explain at least two benefits of recommendation systems to
customers that we discussed in class.</h4>
<ul>
<li><strong>Explanation</strong>: First, the recommendation systems to
customers can reduce search engine cost as customers do not need to
search every option to find what they exactly need, which can make them
overwhelmed with too many options. It also reduces time that customers
have to input to find proper items, making the entire process more
efficient. Second, the first benefit is actually related to this second
benefit which increases customers’ satisfaction. The collaborative
filter can remind users of things that they might want to purchase again
or effectively find products that align with customers’ preferences,
leading to the increase in their satisfaction after using them. Third,
the recommendation systems allow businesses particularly for online
retailers to increase product offerings while increasing larger
inventories as it helps offer the subset of products for users based on
their preferences, requiring niche products to satisfy more broadened
range of customers. For example, Costco often reduces inventories by
offering only products which can meet the preferences for average
customers whereas this recommendation systems allow online retailers to
increase the inventories for niche products which can attract more
diverse customers and broaden their customer segmentation.</li>
</ul>
</div>
<div
id="how-do-recommendation-systems-make-larger-inventories-more-plausible-for-online-retailers"
class="section level4">
<h4>(3) How do recommendation systems make larger inventories more
plausible for online retailers?</h4>
<ul>
<li><strong>Explanation</strong>: The recommendation systems allow
businesses particularly for online retailers to increase product
offerings while increasing larger inventories as it helps offer the
subset of products for users based on their preferences, requiring niche
products to satisfy more broadened range of customers. For example,
Costco often reduces inventories by offering only products which can
meet the preferences for average customers whereas this recommendation
systems allow online retailers to increase the inventories for niche
products which can attract more diverse customers and broaden their
customer segmentation.</li>
</ul>
</div>
<div
id="what-is-the-key-trade-off-the-collaborative-filter-makes-versus-more-sophisticated-models"
class="section level4">
<h4>(4) What is the key trade-off the collaborative filter makes versus
more sophisticated models?</h4>
<ul>
<li><strong>Explanation</strong>: The collaborative filter trade-off the
accuracy in exchange for the speed. The accuracy is higher in
sophisticated models than the collaborative filter whereas the
collaborative filter achieves higher speed which is necessary sometimes
for online retailers to motivate customers to purchase recommended
products.</li>
</ul>
</div>
<div
id="a-major-limitation-of-the-collaborative-filter-is-that-it-implicitly-treats-a-product-not-purchased-by-a-customer-as-unknown-to-them-and-something-to-potentially-be-recommended.-why-might-this-implicit-assumption-be-wrong"
class="section level4">
<h4>(5) A major limitation of the collaborative filter is that it
implicitly treats a product not purchased by a customer as unknown to
them, and something to potentially be recommended. Why might this
implicit assumption be wrong?</h4>
<ul>
<li><strong>Explanation</strong>: The implicit assumption is the
collaborative filter implicitly treats products not purchased by
customers as being unaware to them and something invaluable to be
recommended. However, in reality, customers often know their preferences
on whether to dislike a product before deciding to purchase it. If any
product with lower demand and not purchased by customers, it would mean
that customers dislike the product and decide not to purchase it.</li>
</ul>
</div>
<div id="what-is-algorithmic-bias" class="section level4">
<h4>(6) What is algorithmic bias?</h4>
<ul>
<li><strong>Explanation</strong>: Algorithmic bias generates outcomes
which disadvantage specific protected lines, such as gender, age,
religion, and other socioeconomic status.</li>
</ul>
</div>
<div
id="one-way-algorithmic-bias-can-happen-is-if-human-inputs-are-used-to-train-the-model.-explain-how-algorithmic-bias-might-occur-in-this-case."
class="section level4">
<h4>(7) One-way algorithmic bias can happen is if human inputs are used
to train the model. Explain how algorithmic bias might occur in this
case.</h4>
<ul>
<li><strong>Explanation</strong>: If any model is trained by human
inputs, it is likely to learn about human behaviors or thought towards
protected lines, such as gender, age, disability, race and other
socioeconomic factors. For example, if the model is trained by
user-generated texts and images in SNSs like Instagram, Facebook and
Twitter where users’ biased opinions towards protected lines were
already reflected, there is high possibility that this model is biased
towards the same lines.</li>
</ul>
</div>
<div
id="algorithmic-bias-occurs-even-if-the-algorithm-does-not-explicitly-model-protected-dimensions-like-gender.-in-machine-learning-models-where-a-large-number-of-variables-are-used-to-predict-an-outcome-e.g.-mortgage-default-how-might-this-happen"
class="section level4">
<h4>(8) Algorithmic bias occurs even if the algorithm does not
explicitly model protected dimensions like gender. In machine learning
models where a large number of variables are used to predict an outcome
(e.g., mortgage default), how might this happen?</h4>
<ul>
<li><strong>Explanation</strong>: Even if the algorithm does not
explicitly include any protected dimensions as model variables such as
gender, age, disability and other socioeconomic factors, algorithmic
bias occurs, for example certain racial groups have shown high default
rate from historical data due to some systematic disparities, for
certain racial groups even if the model does not include race as one of
the model variables. If included variables are correlated with this
unexplicit variable such as race, the model can be biased,
disadvantaging certain racial group.</li>
</ul>
</div>
</div>
</div>


<br><br>
<footer>
  <p class="copyright text-muted" align="center">Copyright &copy; 2022 John Appleseed</p>
</footer>


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->

<script>
$(document).ready(function ()  {

    // temporarily add toc-ignore selector to headers for the consistency with Pandoc
    $('.unlisted.unnumbered').addClass('toc-ignore')

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4,h5",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_');
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = false;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
